{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###   权值初始化\n",
    "\n",
    "```\n",
    "\n",
    "在搭建好网络模型之后，一个重要的步骤就是对网络模型中的权值进行初始化。适当的权值初始化可以加快模型的收敛，而不恰当的权值初始化可能引发梯度消失或者梯度爆炸，最终导致模型无法收敛。下面分 3 部分介绍。第一部分介绍不恰当的权值初始化是如何引发梯度消失与梯度爆炸的，第二部分介绍常用的 Xavier 方法与 Kaiming 方法，第三部分介绍 PyTorch 中的 10 种初始化方法。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 梯度消失与梯度爆炸\n",
    "考虑一个 3 层的全连接网络。\n",
    "\n",
    "$H{1}=X \\times W{1}$，$H{2}=H{1} \\times W{2}$，$Out=H{2} \\times W_{3}$\n",
    "\n",
    "![image](https://image.zhangxiann.com/20200630085446.png \"image\")\n",
    "\n",
    "$\\begin{aligned} \\Delta \\mathrm{W}{2} &=\\frac{\\partial \\mathrm{Loss}}{\\partial \\mathrm{W}{2}}=\\frac{\\partial \\mathrm{Loss}}{\\partial \\mathrm{out}}  \\frac{\\partial \\mathrm{out}}{\\partial \\mathrm{H}_{2}}  \\frac{\\partial \\mathrm{H}{2}}{\\partial \\mathrm{w}{2}} \\ &=\\frac{\\partial \\mathrm{Loss}}{\\partial \\mathrm{out}}  \\frac{\\partial \\mathrm{out}}{\\partial \\mathrm{H}_{2}}  \\mathrm{H}_{1} \\end{aligned}$\n",
    "\n",
    "所以$\\Delta \\mathrm{W}{2}$依赖于前一层的输出$H{1}$。如果$H{1}$ 趋近于零，那么$\\Delta \\mathrm{W}{2}$也接近于 0，造成梯度消失。如果$H{1}$ 趋近于无穷大，那么$\\Delta \\mathrm{W}{2}$也接近于无穷大，造成梯度爆炸。要避免梯度爆炸或者梯度消失，就要严格控制网络层输出的数值范围。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "通用函数\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "\n",
    "def transform_invert(img_, transform_train):\n",
    "    \"\"\"\n",
    "    将data 进行反transfrom操作\n",
    "    :param img_: tensor\n",
    "    :param transform_train: torchvision.transforms\n",
    "    :return: PIL image\n",
    "    \"\"\"\n",
    "    if 'Normalize' in str(transform_train):\n",
    "        norm_transform = list(filter(lambda x: isinstance(x, transforms.Normalize), transform_train.transforms))\n",
    "        mean = torch.tensor(norm_transform[0].mean, dtype=img_.dtype, device=img_.device)\n",
    "        std = torch.tensor(norm_transform[0].std, dtype=img_.dtype, device=img_.device)\n",
    "        img_.mul_(std[:, None, None]).add_(mean[:, None, None])\n",
    "\n",
    "    img_ = img_.transpose(0, 2).transpose(0, 1)  # C*H*W --> H*W*C\n",
    "    if 'ToTensor' in str(transform_train):\n",
    "        img_ = img_.detach().numpy() * 255\n",
    "\n",
    "    if img_.shape[2] == 3:\n",
    "        img_ = Image.fromarray(img_.astype('uint8')).convert('RGB')\n",
    "    elif img_.shape[2] == 1:\n",
    "        img_ = Image.fromarray(img_.astype('uint8').squeeze())\n",
    "    else:\n",
    "        raise Exception(\"Invalid img shape, expected 1 or 3 in axis 2, but got {}!\".format(img_.shape[2]) )\n",
    "\n",
    "    return img_\n",
    "\n",
    "\n",
    "def set_seed(seed=1):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# 下面构建 100 层全连接网络，先不适用非线性激活函数，每层的权重初始化为服从$N(0,1)$的正态分布，输出数据使用随机初始化的数据。\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "set_seed(1)  # 设置随机种子\n",
    "\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, neural_num, layers):\n",
    "        super(MLP, self).__init__()\n",
    "        self.linears = nn.ModuleList([nn.Linear(neural_num, neural_num, bias=False) for i in range(layers)])\n",
    "        self.neural_num = neural_num\n",
    "\n",
    "    def forward(self, x):\n",
    "        for (i, linear) in enumerate(self.linears):\n",
    "            x = linear(x)\n",
    "\n",
    "\n",
    "        return x\n",
    "\n",
    "    def initialize(self):\n",
    "        for m in self.modules():\n",
    "            # 判断这一层是否为线性层，如果为线性层则初始化权值\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.normal_(m.weight.data)    # normal: mean=0, std=1\n",
    "\n",
    "layer_nums = 100\n",
    "neural_nums = 256\n",
    "batch_size = 16\n",
    "\n",
    "net = MLP(neural_nums, layer_nums)\n",
    "net.initialize()\n",
    "\n",
    "inputs = torch.randn((batch_size, neural_nums))  # normal: mean=0, std=1\n",
    "\n",
    "output = net(inputs)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "也就是数据太大(梯度爆炸)或者太小(梯度消失)了。接下来我们在forward()函数中判断每一次前向传播的输出的标准差是否为 nan，如果是 nan 则停止前向传播。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer:0, std:15.959932327270508\n",
      "layer:1, std:256.6237487792969\n",
      "layer:2, std:4107.24560546875\n",
      "layer:3, std:65576.8125\n",
      "layer:4, std:1045011.875\n",
      "layer:5, std:17110408.0\n",
      "layer:6, std:275461440.0\n",
      "layer:7, std:4402537984.0\n",
      "layer:8, std:71323615232.0\n",
      "layer:9, std:1148104736768.0\n",
      "layer:10, std:17911758454784.0\n",
      "layer:11, std:283574846619648.0\n",
      "layer:12, std:4480599540629504.0\n",
      "layer:13, std:7.196814275405414e+16\n",
      "layer:14, std:1.1507761512626258e+18\n",
      "layer:15, std:1.853110740188555e+19\n",
      "layer:16, std:2.9677722308204246e+20\n",
      "layer:17, std:4.780376223769898e+21\n",
      "layer:18, std:7.613223480799065e+22\n",
      "layer:19, std:1.2092652108825478e+24\n",
      "layer:20, std:1.9232569606642055e+25\n",
      "layer:21, std:3.134467063655912e+26\n",
      "layer:22, std:5.014437175989598e+27\n",
      "layer:23, std:8.066615144249704e+28\n",
      "layer:24, std:1.2392661553516338e+30\n",
      "layer:25, std:1.9455688099759845e+31\n",
      "layer:26, std:3.0238180658999113e+32\n",
      "layer:27, std:4.950357571077011e+33\n",
      "layer:28, std:8.150924530001331e+34\n",
      "layer:29, std:1.3229830735592165e+36\n",
      "layer:30, std:nan\n",
      "output is nan in 30 layers\n",
      "tensor([[ 9.8051e+36,  2.5191e+37,  8.4358e+36,  ...,  4.1858e+37,\n",
      "         -2.3880e+37, -1.1118e+37],\n",
      "        [-3.6358e+37,  4.5756e+35, -2.7716e+36,  ..., -1.8793e+37,\n",
      "          4.1133e+36, -1.2764e+37],\n",
      "        [ 2.1538e+37, -3.1103e+37,  2.5804e+37,  ...,  6.9849e+36,\n",
      "          3.2139e+37,  4.8493e+36],\n",
      "        ...,\n",
      "        [ 1.3798e+37,  7.6824e+36, -2.9655e+36,  ...,  8.7791e+35,\n",
      "          1.3106e+37,  6.6469e+36],\n",
      "        [ 1.2969e+37,  2.3706e+37, -1.0296e+37,  ...,  1.5095e+37,\n",
      "         -3.8905e+37, -1.1750e+37],\n",
      "        [-8.2960e+36, -8.1296e+36, -7.4200e+36,  ..., -1.9674e+37,\n",
      "         -1.5635e+37,  1.5640e+36]], grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# 下面构建 100 层全连接网络，先不适用非线性激活函数，每层的权重初始化为服从$N(0,1)$的正态分布，输出数据使用随机初始化的数据。\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "set_seed(1)  # 设置随机种子\n",
    "\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, neural_num, layers):\n",
    "        super(MLP, self).__init__()\n",
    "        self.linears = nn.ModuleList([nn.Linear(neural_num, neural_num, bias=False) for i in range(layers)])\n",
    "        self.neural_num = neural_num\n",
    "\n",
    "    def forward(self, x):\n",
    "        for (i, linear) in enumerate(self.linears):\n",
    "            x = linear(x)\n",
    "\n",
    "            print(\"layer:{}, std:{}\".format(i, x.std()))\n",
    "            if torch.isnan(x.std()):\n",
    "                print(\"output is nan in {} layers\".format(i))\n",
    "                break\n",
    "\n",
    "        return x\n",
    "\n",
    "    def initialize(self):\n",
    "        for m in self.modules():\n",
    "            # 判断这一层是否为线性层，如果为线性层则初始化权值\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.normal_(m.weight.data)    # normal: mean=0, std=1\n",
    "\n",
    "layer_nums = 100\n",
    "neural_nums = 256\n",
    "batch_size = 16\n",
    "\n",
    "net = MLP(neural_nums, layer_nums)\n",
    "net.initialize()\n",
    "\n",
    "inputs = torch.randn((batch_size, neural_nums))  # normal: mean=0, std=1\n",
    "\n",
    "output = net(inputs)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上述是没有使用非线性变换的实验结果，如果在forward()中添加非线性变换tanh，每一层的输出方差还是会越来越小，会导致梯度消失。因此出现了 Xavier 初始化方法与 Kaiming 初始化方法。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Xavier 方法与 Kaiming 方法\n",
    "\n",
    "Xavier 方法\n",
    "\n",
    "Xavier 是 2010 年提出的，针对有非线性激活函数时的权值初始化方法，目标是保持数据的方差维持在 1 左右，主要针对饱和激活函数如 sigmoid 和 tanh 等。同时考虑前向传播和反向传播，需要满足两个等式：$\\boldsymbol{n}{\\boldsymbol{i}} * \\boldsymbol{D}(\\boldsymbol{W})=\\mathbf{1}$和$\\boldsymbol{n}{\\boldsymbol{i+1}} * \\boldsymbol{D}(\\boldsymbol{W})=\\mathbf{1}$，可得：$D(W)=\\frac{2}{n{i}+n{i+1}}$。为了使 Xavier 方法初始化的权值服从均匀分布，假设$W$服从均匀分布$U[-a, a]$，那么方差 $D(W)=\\frac{(-a-a)^{2}}{12}=\\frac{(2 a)^{2}}{12}=\\frac{a^{2}}{3}$，令$\\frac{2}{n{i}+n{i+1}}=\\frac{a^{2}}{3}$，解得：$\\boldsymbol{a}=\\frac{\\sqrt{6}}{\\sqrt{n{i}+n{i+1}}}$，所以$W$服从分布$U\\left[-\\frac{\\sqrt{6}}{\\sqrt{n{i}+n{i+1}}}, \\frac{\\sqrt{6}}{\\sqrt{n{i}+n{i+1}}}\\right]$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer:0, std:1.6565721035003662\n",
      "layer:1, std:2.7454750537872314\n",
      "layer:2, std:4.626405239105225\n",
      "layer:3, std:7.675012111663818\n",
      "layer:4, std:13.035057067871094\n",
      "layer:5, std:21.013654708862305\n",
      "layer:6, std:35.22600555419922\n",
      "layer:7, std:59.19900131225586\n",
      "layer:8, std:97.7145767211914\n",
      "layer:9, std:160.1146697998047\n",
      "layer:10, std:268.224853515625\n",
      "layer:11, std:453.31414794921875\n",
      "layer:12, std:764.7789306640625\n",
      "layer:13, std:1246.519287109375\n",
      "layer:14, std:2084.412841796875\n",
      "layer:15, std:3492.153076171875\n",
      "layer:16, std:5920.9140625\n",
      "layer:17, std:9637.7041015625\n",
      "layer:18, std:16439.984375\n",
      "layer:19, std:28287.46484375\n",
      "layer:20, std:46427.4375\n",
      "layer:21, std:77196.75\n",
      "layer:22, std:126378.0234375\n",
      "layer:23, std:211954.15625\n",
      "layer:24, std:346803.1875\n",
      "layer:25, std:582732.3125\n",
      "layer:26, std:981679.0625\n",
      "layer:27, std:1634631.125\n",
      "layer:28, std:2753212.25\n",
      "layer:29, std:4657875.0\n",
      "layer:30, std:7713320.5\n",
      "layer:31, std:12499062.0\n",
      "layer:32, std:21196508.0\n",
      "layer:33, std:35260592.0\n",
      "layer:34, std:59367156.0\n",
      "layer:35, std:102678472.0\n",
      "layer:36, std:166067952.0\n",
      "layer:37, std:277956384.0\n",
      "layer:38, std:463844288.0\n",
      "layer:39, std:783258560.0\n",
      "layer:40, std:1357586688.0\n",
      "layer:41, std:2210034944.0\n",
      "layer:42, std:3683356672.0\n",
      "layer:43, std:6312603648.0\n",
      "layer:44, std:10483742720.0\n",
      "layer:45, std:17285013504.0\n",
      "layer:46, std:29749440512.0\n",
      "layer:47, std:49015828480.0\n",
      "layer:48, std:78760017920.0\n",
      "layer:49, std:137946693632.0\n",
      "layer:50, std:228275355648.0\n",
      "layer:51, std:383087902720.0\n",
      "layer:52, std:632990007296.0\n",
      "layer:53, std:1020743647232.0\n",
      "layer:54, std:1685589721088.0\n",
      "layer:55, std:2736318316544.0\n",
      "layer:56, std:4616465416192.0\n",
      "layer:57, std:7651542958080.0\n",
      "layer:58, std:13171629752320.0\n",
      "layer:59, std:22366006542336.0\n",
      "layer:60, std:37608482668544.0\n",
      "layer:61, std:60872810561536.0\n",
      "layer:62, std:101216440811520.0\n",
      "layer:63, std:161597892853760.0\n",
      "layer:64, std:268199081803776.0\n",
      "layer:65, std:459760646225920.0\n",
      "layer:66, std:755697952227328.0\n",
      "layer:67, std:1269224576122880.0\n",
      "layer:68, std:2111365120524288.0\n",
      "layer:69, std:3565512197931008.0\n",
      "layer:70, std:5874752904232960.0\n",
      "layer:71, std:9921776033202176.0\n",
      "layer:72, std:1.695486121607168e+16\n",
      "layer:73, std:2.816945354388275e+16\n",
      "layer:74, std:4.838476509452698e+16\n",
      "layer:75, std:7.83642552958976e+16\n",
      "layer:76, std:1.2999574078829363e+17\n",
      "layer:77, std:2.1854819108624794e+17\n",
      "layer:78, std:3.6291374139572224e+17\n",
      "layer:79, std:5.76766485255422e+17\n",
      "layer:80, std:9.387235828155023e+17\n",
      "layer:81, std:1.5749926824286618e+18\n",
      "layer:82, std:2.653023426410709e+18\n",
      "layer:83, std:4.3421456906804265e+18\n",
      "layer:84, std:7.140134502533169e+18\n",
      "layer:85, std:1.1530277174300901e+19\n",
      "layer:86, std:1.914357496097538e+19\n",
      "layer:87, std:3.172060919515159e+19\n",
      "layer:88, std:5.265919186405268e+19\n",
      "layer:89, std:8.791104757855106e+19\n",
      "layer:90, std:1.48016607174926e+20\n",
      "layer:91, std:2.4240067322024087e+20\n",
      "layer:92, std:3.976788246209065e+20\n",
      "layer:93, std:6.397032537582155e+20\n",
      "layer:94, std:1.0793217795402655e+21\n",
      "layer:95, std:1.6838061101562578e+21\n",
      "layer:96, std:2.865490135258283e+21\n",
      "layer:97, std:4.93726587218879e+21\n",
      "layer:98, std:8.196872766237706e+21\n",
      "layer:99, std:1.355778268522403e+22\n",
      "tensor([[ 7.3437e+21,  1.7586e+21, -7.9328e+21,  ..., -7.6512e+21,\n",
      "          1.5119e+22, -7.1650e+21],\n",
      "        [-2.2343e+22,  7.2241e+21,  1.3130e+22,  ...,  1.6052e+22,\n",
      "         -2.7013e+22,  5.9734e+21],\n",
      "        [-6.6768e+22,  8.6247e+20, -1.3519e+22,  ...,  9.8350e+21,\n",
      "         -1.8926e+21,  1.8486e+22],\n",
      "        ...,\n",
      "        [-1.3932e+22, -2.4352e+21, -8.9350e+21,  ...,  9.4243e+21,\n",
      "          7.2218e+21,  7.5981e+21],\n",
      "        [-1.6338e+22, -3.9434e+20, -3.1701e+21,  ...,  1.7290e+22,\n",
      "         -1.4177e+21,  9.5233e+21],\n",
      "        [-2.1213e+21,  2.4045e+21, -5.0848e+21,  ...,  4.8403e+21,\n",
      "          8.1973e+20, -1.2208e+21]], grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# 下面构建 100 层全连接网络，先不适用非线性激活函数，每层的权重初始化为服从$N(0,1)$的正态分布，输出数据使用随机初始化的数据。\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "set_seed(1)  # 设置随机种子\n",
    "\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, neural_num, layers):\n",
    "        super(MLP, self).__init__()\n",
    "        self.linears = nn.ModuleList([nn.Linear(neural_num, neural_num, bias=False) for i in range(layers)])\n",
    "        self.neural_num = neural_num\n",
    "\n",
    "    def forward(self, x):\n",
    "        for (i, linear) in enumerate(self.linears):\n",
    "            x = linear(x)\n",
    "\n",
    "            print(\"layer:{}, std:{}\".format(i, x.std()))\n",
    "            if torch.isnan(x.std()):\n",
    "                print(\"output is nan in {} layers\".format(i))\n",
    "                break\n",
    "\n",
    "        return x\n",
    "\n",
    "    def initialize(self):\n",
    "        for m in self.modules():\n",
    "            # 判断这一层是否为线性层，如果为线性层则初始化权值\n",
    "            if isinstance(m, nn.Linear):\n",
    "                a = np.sqrt(6 / (self.neural_num + self.neural_num))\n",
    "                # 把 a 变换到 tanh，计算增益\n",
    "                tanh_gain = nn.init.calculate_gain('tanh')\n",
    "                a *= tanh_gain\n",
    "                nn.init.uniform_(m.weight.data, -a, a)\n",
    "\n",
    "layer_nums = 100\n",
    "neural_nums = 256\n",
    "batch_size = 16\n",
    "\n",
    "net = MLP(neural_nums, layer_nums)\n",
    "net.initialize()\n",
    "\n",
    "inputs = torch.randn((batch_size, neural_nums))  # normal: mean=0, std=1\n",
    "\n",
    "output = net(inputs)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PyTorch 也提供了 Xavier 初始化方法，可以直接调用：\n",
    "```python\n",
    "tanh_gain = nn.init.calculate_gain('tanh')\n",
    "nn.init.xavier_uniform_(m.weight.data, gain=tanh_gain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 常用初始化方法\n",
    "```\n",
    "1.Xavier 均匀分布\n",
    "2.Xavier 正态分布\n",
    "3.Kaiming 均匀分布\n",
    "4.Kaiming 正态分布\n",
    "5.均匀分布\n",
    "6.正态分布\n",
    "7.常数分布\n",
    "8.正交矩阵初始化\n",
    "9.单位矩阵初始化\n",
    "10.稀疏矩阵初始化"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('pytorch_env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6ee8edfac98a68d22388d5c9d2c882fbbce04e225b817d7a8b0ca882dcedeab3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
