{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 损失函数\n",
    "\n",
    "损失函数是衡量模型输出与真实标签之间的差异。我们还经常听到代价函数和目标函数，它们之间差异如下：\n",
    "损失函数(Loss Function)是计算一个样本的模型输出与真实标签的差异\n",
    "Loss $=f\\left(y^{\\wedge}, y\\right)$\n",
    "代价函数(Cost Function)是计算整个样本集的模型输出与真实标签的差异，是所有样本损失函数的平均值\n",
    "$\\cos t=\\frac{1}{N} \\sum{i}^{N} f\\left(y{i}^{\\wedge}, y_{i}\\right)$\n",
    "目标函数(Objective Function)就是代价函数加上正则项"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### nn.CrossEntropyLoss 交叉熵损失函数\n",
    "```python\n",
    "nn.CrossEntropyLoss(weight=None, size_average=None, ignore_index=-100, reduce=None, reduction='mean')\n",
    "```\n",
    "```\n",
    "功能：把nn.LogSoftmax()和nn.NLLLoss()结合，计算交叉熵。nn.LogSoftmax()的作用是把输出值归一化到了 [0,1] 之间。\n",
    "主要参数：\n",
    "    weight：各类别的 loss 设置权值\n",
    "    ignore_index：忽略某个类别的 loss 计算\n",
    "    reduction：计算模式，可以为 none(逐个元素计算)，sum(所有元素求和，返回标量)，mean(加权平均，返回标量)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 基本概念\n",
    "自信息：$\\mathrm{I}(x)=-\\log [p(x)]$\n",
    "\n",
    "信息熵就是求自信息的期望：$\\mathrm{H}(\\mathrm{P})=E{x \\sim p}[I(x)]=-\\sum{i}^{N} P\\left(x{i}\\right) \\log P\\left(x{i}\\right)$\n",
    "\n",
    "相对熵，也被称为 KL 散度，用于衡量两个分布的相似性(距离)：$\\boldsymbol{D}{K L}(\\boldsymbol{P}, \\boldsymbol{Q})=\\boldsymbol{E}{\\boldsymbol{x} \\sim p}\\left[\\log \\frac{\\boldsymbol{P}(\\boldsymbol{x})}{Q(\\boldsymbol{x})}\\right]$。其中$P(X)$是真实分布，$Q(X)$是拟合的分布\n",
    "\n",
    "交叉熵：$\\mathrm{H}(\\boldsymbol{P}, \\boldsymbol{Q})=-\\sum{i=1}^{N} \\boldsymbol{P}\\left(\\boldsymbol{x}{i}\\right) \\log \\boldsymbol{Q}\\left(\\boldsymbol{x}_{i}\\right)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross Entropy Loss:\n",
      "  tensor([1.3133, 0.1269, 0.1269]) tensor(1.5671) tensor(0.5224)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "# fake data\n",
    "inputs = torch.tensor([[1, 2], [1, 3], [1, 3]], dtype=torch.float)\n",
    "target = torch.tensor([0, 1, 1], dtype=torch.long)\n",
    "\n",
    "# def loss function\n",
    "loss_f_none = nn.CrossEntropyLoss(weight=None, reduction='none')\n",
    "loss_f_sum = nn.CrossEntropyLoss(weight=None, reduction='sum')\n",
    "loss_f_mean = nn.CrossEntropyLoss(weight=None, reduction='mean')\n",
    "\n",
    "# forward\n",
    "loss_none = loss_f_none(inputs, target)\n",
    "loss_sum = loss_f_sum(inputs, target)\n",
    "loss_mean = loss_f_mean(inputs, target)\n",
    "\n",
    "# view\n",
    "print(\"Cross Entropy Loss:\\n \", loss_none, loss_sum, loss_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们根据单个样本的 loss 计算公式$\\operatorname{loss}(x, \\text { class })=-\\log \\left(\\frac{\\exp (x[\\text { class }])}{\\sum{j} \\exp (x[j])}\\right)=-x[\\text { class }]+\\log \\left(\\sum{j} \\exp (x[j])\\right)$，可以使用以下代码来手动计算第一个样本的损失"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第一个样本loss为:  1.3132617\n"
     ]
    }
   ],
   "source": [
    "idx = 0\n",
    "\n",
    "input_1 = inputs.detach().numpy()[idx]      # [1, 2]\n",
    "target_1 = target.numpy()[idx]              # [0]\n",
    "\n",
    "# 第一项\n",
    "x_class = input_1[target_1]\n",
    "\n",
    "# 第二项\n",
    "sigma_exp_x = np.sum(list(map(np.exp, input_1)))\n",
    "log_sigma_exp_x = np.log(sigma_exp_x)\n",
    "\n",
    "# 输出loss\n",
    "loss_1 = -x_class + log_sigma_exp_x\n",
    "\n",
    "print(\"第一个样本loss为: \", loss_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### nn.NLLLoss\n",
    "```python\n",
    "nn.NLLLoss(weight=None, size_average=None, ignore_index=-100, reduce=None, reduction='mean')\n",
    "```\n",
    "```\n",
    "功能：实现负对数似然函数中的符号功能\n",
    "主要参数：\n",
    "    weight：各类别的 loss 权值设置\n",
    "    ignore_index：忽略某个类别\n",
    "    reduction：计算模式，，可以为 none(逐个元素计算)，sum(所有元素求和，返回标量)，mean(加权平均，返回标量)\n",
    "```\n",
    "每个样本的 loss 公式为：$l{n}=-w{y{n}} x{n, y_{n}}$。还是使用上面的例子，第一个样本的输出为 [1,2]，类别为 0，则第一个样本的 loss 为 -1；第一个样本的输出为 [1,3]，类别为 1，则第一个样本的 loss 为 -3。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "weights:  tensor([1., 1.])\n",
      "NLL Loss tensor([-1., -3., -3.]) tensor(-7.) tensor(-2.3333)\n"
     ]
    }
   ],
   "source": [
    "weights = torch.tensor([1, 1], dtype=torch.float)\n",
    "\n",
    "loss_f_none_w = nn.NLLLoss(weight=weights, reduction='none')\n",
    "loss_f_sum = nn.NLLLoss(weight=weights, reduction='sum')\n",
    "loss_f_mean = nn.NLLLoss(weight=weights, reduction='mean')\n",
    "\n",
    "# forward\n",
    "loss_none_w = loss_f_none_w(inputs, target)\n",
    "loss_sum = loss_f_sum(inputs, target)\n",
    "loss_mean = loss_f_mean(inputs, target)\n",
    "\n",
    "# view\n",
    "print(\"\\nweights: \", weights)\n",
    "print(\"NLL Loss\", loss_none_w, loss_sum, loss_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### nn.BCELoss\n",
    "```python\n",
    "nn.BCELoss(weight=None, size_average=None, reduce=None, reduction='mean')\n",
    "```\n",
    "```\n",
    "功能：计算二分类的交叉熵。需要注意的是：输出值区间为 [0,1]。\n",
    "主要参数：\n",
    "    weight：各类别的 loss 权值设置\n",
    "    ignore_index：忽略某个类别\n",
    "    reduction：计算模式，，可以为 none(逐个元素计算)，sum(所有元素求和，返回标量)，mean(加权平均，返回标量)\n",
    "```\n",
    "计算公式为：$l{n}=-w{n}\\left[y{n} \\cdot \\log x{n}+\\left(1-y{n}\\right) \\cdot \\log \\left(1-x{n}\\right)\\right]$\n",
    "\n",
    "```\n",
    "使用这个函数有两个不同的地方：\n",
    "预测的标签需要经过 sigmoid 变换到 [0,1] 之间。\n",
    "真实的标签需要转换为 one hot 向量，类型为torch.float。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "weights:  tensor([1., 1.])\n",
      "BCE Loss tensor([[0.3133, 2.1269],\n",
      "        [0.1269, 2.1269],\n",
      "        [3.0486, 0.0181],\n",
      "        [4.0181, 0.0067]]) tensor(11.7856) tensor(1.4732)\n"
     ]
    }
   ],
   "source": [
    "inputs = torch.tensor([[1, 2], [2, 2], [3, 4], [4, 5]], dtype=torch.float)\n",
    "target = torch.tensor([[1, 0], [1, 0], [0, 1], [0, 1]], dtype=torch.float)\n",
    "\n",
    "target_bce = target\n",
    "\n",
    "# itarget\n",
    "inputs = torch.sigmoid(inputs)\n",
    "\n",
    "weights = torch.tensor([1, 1], dtype=torch.float)\n",
    "\n",
    "loss_f_none_w = nn.BCELoss(weight=weights, reduction='none')\n",
    "loss_f_sum = nn.BCELoss(weight=weights, reduction='sum')\n",
    "loss_f_mean = nn.BCELoss(weight=weights, reduction='mean')\n",
    "\n",
    "# forward\n",
    "loss_none_w = loss_f_none_w(inputs, target_bce)\n",
    "loss_sum = loss_f_sum(inputs, target_bce)\n",
    "loss_mean = loss_f_mean(inputs, target_bce)\n",
    "\n",
    "# view\n",
    "print(\"\\nweights: \", weights)\n",
    "print(\"BCE Loss\", loss_none_w, loss_sum, loss_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### nn.BCEWithLogitsLoss\n",
    "```python\n",
    "nn.BCEWithLogitsLoss(weight=None, size_average=None, reduce=None, reduction='mean', pos_weight=None)\n",
    "```\n",
    "```\n",
    "功能：结合 sigmoid 与二分类交叉熵。需要注意的是，网络最后的输出不用经过 sigmoid 函数。这个 loss 出现的原因是有时网络模型最后一层输出不希望是归一化到 [0,1] 之间，但是在计算 loss 时又需要归一化到 [0,1] 之间。\n",
    "主要参数：\n",
    "    weight：各类别的 loss 权值设置\n",
    "    pos_weight：设置样本类别对应的神经元的输出的 loss 权值\n",
    "    ignore_index：忽略某个类别\n",
    "    reduction：计算模式，，可以为 none(逐个元素计算)，sum(所有元素求和，返回标量)，mean(加权平均，返回标量)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "pos_weights:  tensor([3.])\n",
      "tensor([[0.9398, 2.1269],\n",
      "        [0.3808, 2.1269],\n",
      "        [3.0486, 0.0544],\n",
      "        [4.0181, 0.0201]]) tensor(12.7158) tensor(1.5895)\n"
     ]
    }
   ],
   "source": [
    "inputs = torch.tensor([[1, 2], [2, 2], [3, 4], [4, 5]], dtype=torch.float)\n",
    "target = torch.tensor([[1, 0], [1, 0], [0, 1], [0, 1]], dtype=torch.float)\n",
    "\n",
    "target_bce = target\n",
    "\n",
    "# itarget\n",
    "# inputs = torch.sigmoid(inputs)\n",
    "\n",
    "weights = torch.tensor([1], dtype=torch.float)\n",
    "pos_w = torch.tensor([3], dtype=torch.float)        # 3\n",
    "\n",
    "loss_f_none_w = nn.BCEWithLogitsLoss(weight=weights, reduction='none', pos_weight=pos_w)\n",
    "loss_f_sum = nn.BCEWithLogitsLoss(weight=weights, reduction='sum', pos_weight=pos_w)\n",
    "loss_f_mean = nn.BCEWithLogitsLoss(weight=weights, reduction='mean', pos_weight=pos_w)\n",
    "\n",
    "# forward\n",
    "loss_none_w = loss_f_none_w(inputs, target_bce)\n",
    "loss_sum = loss_f_sum(inputs, target_bce)\n",
    "loss_mean = loss_f_mean(inputs, target_bce)\n",
    "\n",
    "# view\n",
    "print(\"\\npos_weights: \", pos_w)\n",
    "print(loss_none_w, loss_sum, loss_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### nn.L1Loss\n",
    "```python\n",
    "nn.L1Loss(size_average=None, reduce=None, reduction='mean')\n",
    "```\n",
    "```\n",
    "功能：计算 inputs 与 target 之差的绝对值\n",
    "主要参数：\n",
    "    reduction：计算模式，，可以为 none(逐个元素计算)，sum(所有元素求和，返回标量)，mean(加权平均，返回标量)\n",
    "```\n",
    "公式：$l{n}=\\left|x{n}-y_{n}\\right|$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### nn.MSELoss\n",
    "功能：计算 inputs 与 target 之差的平方\n",
    "\n",
    "公式：$l{n}=\\left(x{n}-y_{n}\\right)^{2}$\n",
    "\n",
    "主要参数：\n",
    "    reduction：计算模式，，可以为 none(逐个元素计算)，sum(所有元素求和，返回标量)，mean(加权平均，返回标量)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input:tensor([[1., 1.],\n",
      "        [1., 1.]])\n",
      "target:tensor([[3., 3.],\n",
      "        [3., 3.]])\n",
      "L1 loss:tensor([[2., 2.],\n",
      "        [2., 2.]])\n",
      "MSE loss:tensor([[4., 4.],\n",
      "        [4., 4.]])\n"
     ]
    }
   ],
   "source": [
    "inputs = torch.ones((2, 2))\n",
    "target = torch.ones((2, 2)) * 3\n",
    "\n",
    "loss_f = nn.L1Loss(reduction='none')\n",
    "loss = loss_f(inputs, target)\n",
    "\n",
    "print(\"input:{}\\ntarget:{}\\nL1 loss:{}\".format(inputs, target, loss))\n",
    "\n",
    "# ------------------------------------------------- 6 MSE loss ----------------------------------------------\n",
    "\n",
    "loss_f_mse = nn.MSELoss(reduction='none')\n",
    "loss_mse = loss_f_mse(inputs, target)\n",
    "\n",
    "print(\"MSE loss:{}\".format(loss_mse))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### nn.SmoothL1Loss\n",
    "```python\n",
    "nn.SmoothL1Loss(size_average=None, reduce=None, reduction='mean')\n",
    "```\n",
    "```\n",
    "功能：平滑的 L1Loss\n",
    "```\n",
    "![image](https://image.zhangxiann.com/20200701101230.png \"image\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### nn.PoissonNLLLoss\n",
    "```python\n",
    "nn.PoissonNLLLoss(log_input=True, full=False, size_average=None, eps=1e-08, reduce=None, reduction='mean')\n",
    "```\n",
    "```\n",
    "主要参数：\n",
    "    log_input：输入是否为对数形式，决定计算公式\n",
    "    当 log_input =  True，表示输入数据已经是经过对数运算之后的，loss(input, target) = exp(input) - target * input\n",
    "    当 log_input =  False，，表示输入数据还没有取对数，loss(input, target) = input - target * log(input+eps)\n",
    "    full：计算所有 loss，默认为 loss\n",
    "    eps：修正项，避免 log(input) 为 nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input:tensor([[ 0.3757,  1.0015],\n",
      "        [-1.4026, -0.6755]])\n",
      "target:tensor([[-0.6639,  1.4299],\n",
      "        [-0.2810,  0.1337]])\n",
      "Poisson NLL loss:tensor([[ 1.7055,  1.2903],\n",
      "        [-0.1481,  0.5992]])\n"
     ]
    }
   ],
   "source": [
    "inputs = torch.randn((2, 2))\n",
    "target = torch.randn((2, 2))\n",
    "\n",
    "loss_f = nn.PoissonNLLLoss(log_input=True, full=False, reduction='none')\n",
    "loss = loss_f(inputs, target)\n",
    "print(\"input:{}\\ntarget:{}\\nPoisson NLL loss:{}\".format(inputs, target, loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### nn.KLDivLoss\n",
    "```python\n",
    "nn.KLDivLoss(size_average=None, reduce=None, reduction='mean')\n",
    "```\n",
    "```\n",
    "功能：计算 KLD(divergence)，KL 散度，相对熵\n",
    "注意事项：需要提前将输入计算 log-probabilities，如通过nn.logsoftmax()\n",
    "主要参数：\n",
    "    reduction：计算模式，，可以为 none(逐个元素计算)，sum(所有元素求和，返回标量)，mean(加权平均，返回标量)，batchmean(batchsize 维度求平均值)\n",
    "```\n",
    "公式：$\\begin{aligned} D{K L}(P | Q)=E{x-p}\\left[\\log \\frac{P(x)}{Q(x)}\\right] &=E{x-p}[\\log P(x)-\\log Q(x)] =\\sum{i=1}^{N} P\\left(x{i}\\right)\\left(\\log P\\left(x{i}\\right)-\\log Q\\left(x_{i}\\right)\\right) \\end{aligned}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss_none:\n",
      "tensor([[-0.5448, -0.1648, -0.1598],\n",
      "        [-0.2503, -0.4597, -0.4219]])\n",
      "loss_mean:\n",
      "-0.3335360288619995\n",
      "loss_bs_mean:\n",
      "-1.0006080865859985\n"
     ]
    }
   ],
   "source": [
    "inputs = torch.tensor([[0.5, 0.3, 0.2], [0.2, 0.3, 0.5]])\n",
    "inputs_log = torch.log(inputs)\n",
    "target = torch.tensor([[0.9, 0.05, 0.05], [0.1, 0.7, 0.2]], dtype=torch.float)\n",
    "\n",
    "loss_f_none = nn.KLDivLoss(reduction='none')\n",
    "loss_f_mean = nn.KLDivLoss(reduction='mean')\n",
    "loss_f_bs_mean = nn.KLDivLoss(reduction='batchmean')\n",
    "\n",
    "loss_none = loss_f_none(inputs, target)\n",
    "loss_mean = loss_f_mean(inputs, target)\n",
    "loss_bs_mean = loss_f_bs_mean(inputs, target)\n",
    "\n",
    "print(\"loss_none:\\n{}\\nloss_mean:\\n{}\\nloss_bs_mean:\\n{}\".format(loss_none, loss_mean, loss_bs_mean))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### nn.MarginRankingLoss\n",
    "```python\n",
    "nn.MarginRankingLoss(margin=0.0, size_average=None, reduce=None, reduction='mean')\n",
    "```\n",
    "功能：计算两个向量之间的相似度，用于排序任务\n",
    "特别说明：该方法计算 两组数据之间的差异，返回一个$n \\times n$ 的 loss 矩阵\n",
    "主要参数：\n",
    "    margin：边界值，$x{1}$与$x{2}$之间的差异值\n",
    "    reduction：计算模式，，可以为 none(逐个元素计算)，sum(所有元素求和，返回标量)，mean(加权平均，返回标量)\n",
    "\n",
    "计算公式：$\\operatorname{loss}(x, y)=\\max (0,-y *(x 1-x 2)+\\operatorname{margin})$，$y$的取值有 +1 和 -1。\n",
    "\n",
    "当 $y=1$时，希望$x{1} > x{2}$，当$x{1} > x{2}$，不产生 loss\n",
    "\n",
    "当 $y=-1$时，希望$x{1} < x{2}$，当$x{1} < x{2}$，不产生 loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 0.],\n",
      "        [0., 0.],\n",
      "        [1., 0.]])\n"
     ]
    }
   ],
   "source": [
    "x1 = torch.tensor([[1,2], [2,2], [3,2]], dtype=torch.float)\n",
    "x2 = torch.tensor([[2,2], [2,2], [2,2]], dtype=torch.float)\n",
    "\n",
    "target = torch.tensor([[1], [1], [-1]], dtype=torch.float)\n",
    "\n",
    "loss_f_none = nn.MarginRankingLoss(margin=0, reduction='none')\n",
    "\n",
    "loss = loss_f_none(x1, x2, target)\n",
    "\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### nn.MultiLabelMarginLoss\n",
    "```python\n",
    "nn.MultiLabelMarginLoss(size_average=None, reduce=None, reduction='mean')\n",
    "```\n",
    "```\n",
    "功能：多标签边界损失函数\n",
    "举例：4 分类任务，样本 x 属于 0 类和 3 类，那么标签为 [0, 3, -1, -1]，\n",
    "主要参数：\n",
    "    reduction：计算模式，，可以为 none(逐个元素计算)，sum(所有元素求和，返回标量)，mean(加权平均，返回标量)\n",
    "\n",
    "```\n",
    "计算公式：$\\operatorname{loss}(x, y)=\\sum_{i j} \\frac{\\max (0,1-(x[y[j]]-x[i]))}{x \\cdot \\operatorname{size}(0)}$，表示每个真实类别的神经元输出减去其他神经元的输出。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.8500])\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([[0.1, 0.2, 0.4, 0.8]])\n",
    "y = torch.tensor([[0, 3, -1, -1]], dtype=torch.long)\n",
    "\n",
    "loss_f = nn.MultiLabelMarginLoss(reduction='none')\n",
    "\n",
    "loss = loss_f(x, y)\n",
    "\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### nn.SoftMarginLoss\n",
    "```python\n",
    "nn.SoftMarginLoss(size_average=None, reduce=None, reduction='mean')\n",
    "```\n",
    "```\n",
    "功能：计算二分类的 logistic 损失\n",
    "主要参数：\n",
    "    reduction：计算模式，，可以为 none(逐个元素计算)，sum(所有元素求和，返回标量)，mean(加权平均，返回标量)\n",
    "```\n",
    "计算公式：$\\operatorname{loss}(x, y)=\\sum_{i} \\frac{\\log (1+\\exp (-y[i] * x[i]))}{\\text { x.nelement } 0}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SoftMargin:  tensor([[0.8544, 0.4032],\n",
      "        [0.4741, 0.9741]])\n"
     ]
    }
   ],
   "source": [
    "inputs = torch.tensor([[0.3, 0.7], \n",
    "                       [0.5, 0.5]])\n",
    "target = torch.tensor([[-1, 1], [1, -1]], dtype=torch.float)\n",
    "\n",
    "loss_f = nn.SoftMarginLoss(reduction='none')\n",
    "\n",
    "loss = loss_f(inputs, target)\n",
    "\n",
    "print(\"SoftMargin: \", loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### nn.MultiLabelSoftMarginLoss\n",
    "```python\n",
    "nn.MultiLabelSoftMarginLoss(weight=None, size_average=None, reduce=None, reduction='mean')\n",
    "```\n",
    "```\n",
    "功能：SoftMarginLoss 的多标签版本\n",
    "主要参数：\n",
    "    weight：各类别的 loss 权值设置\n",
    "    reduction：计算模式，，可以为 none(逐个元素计算)，sum(所有元素求和，返回标量)，mean(加权平均，返回标量)\n",
    "```\n",
    "计算公式：$\\operatorname{loss}(x, y)=-\\frac{1}{C}  \\sum_{i} y[i]  \\log \\left((1+\\exp (-x[i]))^{-1}\\right)+(1-y[i]) * \\log \\left(\\frac{\\exp (-x[i])}{(1+\\exp (-x[i]))}\\right)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultiLabel SoftMargin:  tensor([0.5429])\n"
     ]
    }
   ],
   "source": [
    "inputs = torch.tensor([[0.3, 0.7, 0.8]])\n",
    "target = torch.tensor([[0, 1, 1]], dtype=torch.float)\n",
    "\n",
    "loss_f = nn.MultiLabelSoftMarginLoss(reduction='none')\n",
    "\n",
    "loss = loss_f(inputs, target)\n",
    "\n",
    "print(\"MultiLabel SoftMargin: \", loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### nn.MultiMarginLoss\n",
    "```python\n",
    "nn.MultiMarginLoss(p=1, margin=1.0, weight=None, size_average=None, reduce=None, reduction='mean')\n",
    "```\n",
    "```\n",
    "功能：计算多分类的折页损失\n",
    "主要参数：\n",
    "    p：可以选择 1 或 2\n",
    "    weight：各类别的 loss 权值设置\n",
    "    margin：边界值\n",
    "    reduction：计算模式，，可以为 none(逐个元素计算)，sum(所有元素求和，返回标量)，mean(加权平均，返回标量)\n",
    "```\n",
    "计算公式：$\\operatorname{loss}(x, y)=\\frac{\\left.\\sum_{i} \\max (0, \\operatorname{margin}-x[y]+x[i])\\right)^{p}}{\\quad \\text { x.size }(0)}$，其中 y 表示真实标签对应的神经元输出，x 表示其他神经元的输出。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multi Margin Loss:  tensor([0.8000, 0.7000])\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([[0.1, 0.2, 0.7], [0.2, 0.5, 0.3]])\n",
    "y = torch.tensor([1, 2], dtype=torch.long)\n",
    "\n",
    "loss_f = nn.MultiMarginLoss(reduction='none')\n",
    "\n",
    "loss = loss_f(x, y)\n",
    "\n",
    "print(\"Multi Margin Loss: \", loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### nn.TripletMarginLoss\n",
    "```\n",
    "功能：计算三元组损失，人脸验证中常用\n",
    "主要参数：\n",
    "    p：范数的阶，默认为 2\n",
    "    margin：边界值\n",
    "    reduction：计算模式，，可以为 none(逐个元素计算)，sum(所有元素求和，返回标量)，mean(加权平均，返回标量)\n",
    "```\n",
    "计算公式：$L(a, p, n)=\\max \\left{d\\left(a{i}, p{i}\\right)-d\\left(a{i}, n{i}\\right)+\\text { margin, } 0\\right}$，$d\\left(x{i}, y{i}\\right)=\\left|\\mathbf{x}{i}-\\mathbf{y}{i}\\right|{p}$，其中$d(a{i}, p{i})$表示正样本对之间的距离(距离计算公式与 p 有关)，$d(a{i}, n_{i})$表示负样本对之间的距离。表示正样本对之间的距离比负样本对之间的距离小 margin，就没有了 loss。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Triplet Margin Loss tensor(1.5000)\n"
     ]
    }
   ],
   "source": [
    "anchor = torch.tensor([[1.]])\n",
    "pos = torch.tensor([[2.]])\n",
    "neg = torch.tensor([[0.5]])\n",
    "\n",
    "loss_f = nn.TripletMarginLoss(margin=1.0, p=1)\n",
    "\n",
    "loss = loss_f(anchor, pos, neg)\n",
    "\n",
    "print(\"Triplet Margin Loss\", loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### nn.HingeEmbeddingLoss\n",
    "```python\n",
    "nn.HingeEmbeddingLoss(margin=1.0, size_average=None, reduce=None, reduction='mean')\n",
    "```\n",
    "```\n",
    "功能：计算两个输入的相似性，常用于非线性 embedding 和半监督学习\n",
    "特别注意：输入 x 应该为两个输入之差的绝对值\n",
    "主要参数：\n",
    "    margin：边界值\n",
    "    reduction：计算模式，，可以为 none(逐个元素计算)，sum(所有元素求和，返回标量)，mean(加权平均，返回标量)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hinge Embedding Loss tensor([[1.0000, 0.8000, 0.5000]])\n"
     ]
    }
   ],
   "source": [
    "inputs = torch.tensor([[1., 0.8, 0.5]])\n",
    "target = torch.tensor([[1, 1, -1]])\n",
    "\n",
    "loss_f = nn.HingeEmbeddingLoss(margin=1, reduction='none')\n",
    "\n",
    "loss = loss_f(inputs, target)\n",
    "\n",
    "print(\"Hinge Embedding Loss\", loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### nn.CosineEmbeddingLoss\n",
    "```python\n",
    "torch.nn.CosineEmbeddingLoss(margin=0.0, size_average=None, reduce=None, reduction='mean')\n",
    "```\n",
    "```\n",
    "功能：采用余弦相似度计算两个输入的相似性\n",
    "主要参数：\n",
    "    margin：边界值，可取值 [-1, 1]，推荐为 [0, 0.5]\n",
    "    reduction：计算模式，，可以为 none(逐个元素计算)，sum(所有元素求和，返回标量)，mean(加权平均，返回标量)\n",
    "```\n",
    "计算公式：$\\operatorname{loss}(x, y)=\\left{\\begin{array}{ll}1-\\cos \\left(x{1}, x{2}\\right), & \\text { if } y=1 \\ \\max \\left(0, \\cos \\left(x{1}, x{2}\\right)-\\operatorname{margin}\\right), & \\text { if } y=-1\\end{array} \\right.$\n",
    "其中$\\cos (\\theta)=\\frac{A \\cdot B}{|A||B|}=\\frac{\\sum{i=1}^{n} A{i} \\times B{i}}{\\sqrt{\\sum{i=1}^{n}\\left(A{i}\\right)^{2}} \\times \\sqrt{\\sum{i=1}^{n}\\left(B_{i}\\right)^{2}}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = torch.tensor([[0.3, 0.5, 0.7], [0.3, 0.5, 0.7]])\n",
    "x2 = torch.tensor([[0.1, 0.3, 0.5], [0.1, 0.3, 0.5]])\n",
    "\n",
    "target = torch.tensor([[1, -1]], dtype=torch.float)\n",
    "\n",
    "loss_f = nn.CosineEmbeddingLoss(margin=0., reduction='none')\n",
    "\n",
    "# loss = loss_f(x1, x2, target)\n",
    "\n",
    "# print(\"Cosine Embedding Loss\", loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### nn.CTCLoss\n",
    "```python\n",
    "nn.CTCLoss(blank=0, reduction='mean', zero_infinity=False)\n",
    "```\n",
    "```\n",
    "功能：计算 CTC 损失，解决时序类数据的分类，全称为 Connectionist Temporal Classification\n",
    "主要参数：\n",
    "    blank：blank label\n",
    "    zero_infinity：无穷大的值或梯度置 0\n",
    "    reduction：计算模式，，可以为 none(逐个元素计算)，sum(所有元素求和，返回标量)，mean(加权平均，返回标量)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('pytorch_env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6ee8edfac98a68d22388d5c9d2c882fbbce04e225b817d7a8b0ca882dcedeab3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
