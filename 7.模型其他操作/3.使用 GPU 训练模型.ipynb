{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "在数据运算时，两个数据进行运算，那么它们必须同时存放在同一个设备，要么同时是 CPU，要么同时是 GPU。而且数据和模型都要在同一个设备上。数据和模型可以使用to()方法从一个设备转移到另一个设备。而数据的to()方法还可以转换数据类型。\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 从 CPU 到 GPU\n",
    "```python\n",
    "    device = torch.device(\"cuda\")\n",
    "    tensor = tensor.to(device)\n",
    "    module.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 从 GPU 到 CPU\n",
    "```python\n",
    "    device = torch.device(cpu)\n",
    "    tensor = tensor.to(\"cpu\")\n",
    "    module.to(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "tensor和module的 to()方法的区别是：tensor.to()执行的不是 inplace 操作，因此需要赋值；module.to()执行的是 inplace 操作。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 下面的代码是转换数据类型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.float32\n",
      "torch.float64\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "x = torch.ones((3,3))\n",
    "print(x.dtype)\n",
    "x = x.to(torch.float64)\n",
    "print(x.dtype)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tensor.to() 和 module.to()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "# 首先导入库，获取 GPU 的 device\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_cpu:\n",
      "device: cpu is_cuda: False id: 4456792208\n",
      "x_gpu:\n",
      "device: cpu is_cuda: False id: 4456792208\n"
     ]
    }
   ],
   "source": [
    "# 下面的代码是执行Tensor的to()方法\n",
    "x_cpu = torch.ones((3, 3))\n",
    "\n",
    "print(\"x_cpu:\\ndevice: {} is_cuda: {} id: {}\".format(x_cpu.device, x_cpu.is_cuda, id(x_cpu)))\n",
    "\n",
    "x_gpu = x_cpu.to(device)\n",
    "print(\"x_gpu:\\ndevice: {} is_cuda: {} id: {}\".format(x_gpu.device, x_gpu.is_cuda, id(x_gpu)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### torch.cuda常用方法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "torch.cuda.device_count()：返回当前可见可用的 GPU 数量\n",
    "torch.cuda.get_device_name()：获取 GPU 名称\n",
    "torch.cuda.manual_seed()：为当前 GPU 设置随机种子\n",
    "torch.cuda.manual_seed_all()：为所有可见 GPU 设置随机种子\n",
    "torch.cuda.set_device()：设置主 GPU 为哪一个物理 GPU，此方法不推荐使用\n",
    "os.environ.setdefault(\"CUDA_VISIBLE_DEVICES\", \"2\", \"3\")：设置可见 GPU\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 在 PyTorch 中，有物理 GPU 可以逻辑 GPU 之分，可以设置它们之间的对应关系。\n",
    "![image](https://image.zhangxiann.com/20200707194809.png \"image\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "在上图中，如果执行了os.environ.setdefault(\"CUDA_VISIBLE_DEVICES\", \"2\", \"3\")，那么可见 GPU 数量只有 2 个。对应关系如下：\n",
    "逻辑 GPU    gpu0    gpu1\n",
    "物理 GPU    gpu2    gpu3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "如果执行了os.environ.setdefault(\"CUDA_VISIBLE_DEVICES\", \"0\", \"3\", \"2\")，那么可见 GPU 数量只有 3 个。对应关系如下：\n",
    "逻辑 GPU    gpu0    gpu1    gpu2\n",
    "物理 GPU    gpu0    gpu3    gpu2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 多 GPU 的分发并行\n",
    "```python\n",
    "torch.nn.DataParallel(module, device_ids=None, output_device=None, dim=0)\n",
    "```\n",
    "```\n",
    "功能：包装模型，实现分发并行机制。可以把数据平均分发到各个 GPU 上，每个 GPU 实际的数据量为 $\\frac{batch_size}{GPU 数量}$，实现并行计算。\n",
    "```\n",
    "```\n",
    "主要参数：\n",
    "    module：需要包装分发的模型\n",
    "    device_ids：可分发的 GPU，默认分发到所有可见可用的 GPU\n",
    "    output_device：结果输出设备"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 需要注意的是：使用 DataParallel 时，device 要指定某个 GPU 为 主 GPU，否则会报错：\n",
    "RuntimeError: module must have its parameters and buffers on device cuda:1 (device_ids[0]) but found one of them on device: cuda:2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这是因为，使用多 GPU 需要有一个主 GPU，来把每个 batch 的数据分发到每个 GPU，并从每个 GPU 收集计算好的结果。如果不指定主 GPU，那么数据就直接分发到每个 GPU，会造成有些数据在某个 GPU，而另一部分数据在其他 GPU，计算出错。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 下面的代码设置两个可见 GPU，batch_size 为 2，那么每个 GPU 每个 batch 拿到的数据数量为 8，在模型的前向传播中打印数据的数量。\n",
    "```python\n",
    "    # 设置 2 个可见 GPU\n",
    "    gpu_list = [0,1]\n",
    "    gpu_list_str = ','.join(map(str, gpu_list))\n",
    "    os.environ.setdefault(\"CUDA_VISIBLE_DEVICES\", gpu_list_str)\n",
    "    # 这里注意，需要指定一个 GPU 作为主 GPU。\n",
    "    # 否则会报错：module must have its parameters and buffers on device cuda:1 (device_ids[0]) but found one of them on device: cuda:2\n",
    "    # 参考：https://stackoverflow.com/questions/59249563/runtimeerror-module-must-have-its-parameters-and-buffers-on-device-cuda1-devi\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    batch_size = 16\n",
    "\n",
    "    # data\n",
    "    inputs = torch.randn(batch_size, 3)\n",
    "    labels = torch.randn(batch_size, 3)\n",
    "\n",
    "    inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "    # model\n",
    "    net = FooNet(neural_num=3, layers=3)\n",
    "    net = nn.DataParallel(net)\n",
    "    net.to(device)\n",
    "\n",
    "    # training\n",
    "    for epoch in range(1):\n",
    "\n",
    "        outputs = net(inputs)\n",
    "\n",
    "        print(\"model outputs.size: {}\".format(outputs.size()))\n",
    "\n",
    "    print(\"CUDA_VISIBLE_DEVICES :{}\".format(os.environ[\"CUDA_VISIBLE_DEVICES\"]))\n",
    "    print(\"device_count :{}\".format(torch.cuda.device_count()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 下面的代码是根据 GPU 剩余内存来排序。\n",
    "```python\n",
    "    def get_gpu_memory():\n",
    "        import platform\n",
    "        if 'Windows' != platform.system():\n",
    "            import os\n",
    "            os.system('nvidia-smi -q -d Memory | grep -A4 GPU | grep Free > tmp.txt')\n",
    "            memory_gpu = [int(x.split()[2]) for x in open('tmp.txt', 'r').readlines()]\n",
    "            os.system('rm tmp.txt')\n",
    "        else:\n",
    "            memory_gpu = False\n",
    "            print(\"显存计算功能暂不支持windows操作系统\")\n",
    "        return memory_gpu\n",
    "\n",
    "\n",
    "    gpu_memory = get_gpu_memory()\n",
    "    if not gpu_memory:\n",
    "        print(\"\\ngpu free memory: {}\".format(gpu_memory))\n",
    "        gpu_list = np.argsort(gpu_memory)[::-1]\n",
    "\n",
    "        gpu_list_str = ','.join(map(str, gpu_list))\n",
    "        os.environ.setdefault(\"CUDA_VISIBLE_DEVICES\", gpu_list_str)\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "其中nvidia-smi -q -d Memory是查询所有 GPU 的内存信息，-q表示查询，-d是指定查询的内容。\n",
    "nvidia-smi -q -d Memory | grep -A4 GPU是截取 GPU 开始的 4 行，如下\n",
    "\n",
    "nvidia-smi -q -d Memory | grep -A4 GPU | grep Free是提取Free所在的行，也就是提取剩余内存的信息，如下：\n",
    "\n",
    "nvidia-smi -q -d Memory | grep -A4 GPU | grep Free > tmp.txt是把剩余内存的信息保存到tmp.txt中。\n",
    "\n",
    "[int(x.split()[2]) for x in open('tmp.txt', 'r').readlines()]是用列表表达式对每行进行处理。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GPU详细内容链接\n",
    "\n",
    "https://pytorch.zhangxiann.com/7-mo-xing-qi-ta-cao-zuo/7.3-shi-yong-gpu-xun-lian-mo-xing#duo-gpu-de-fen-fa-bing-hang"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('pytorch_env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6ee8edfac98a68d22388d5c9d2c882fbbce04e225b817d7a8b0ca882dcedeab3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
