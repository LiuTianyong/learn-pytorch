{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  模型 Finetune"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "迁移学习：把在 source domain 任务上的学习到的模型应用到 target domain 的任务。\n",
    "\n",
    "Finetune 就是一种迁移学习的方法。比如做人脸识别，可以把 ImageNet 看作 source domain，人脸数据集看作 target domain。通常来说 source domain 要比 target domain 大得多。可以利用 ImageNet 训练好的网络应用到人脸识别中。\n",
    "\n",
    "对于一个模型，通常可以分为前面的 feature extractor (卷积层)和后面的 classifier，在 Finetune 时，通常不改变 feature extractor 的权值，也就是冻结卷积层；并且改变最后一个全连接层的输出来适应目标任务，训练后面 classifier 的权值，这就是 Finetune。通常 target domain 的数据比较小，不足以训练全部参数，容易导致过拟合，因此不改变 feature extractor 的权值。\n",
    "\n",
    "Finetune 步骤如下：\n",
    "    1.获取预训练模型的参数\n",
    "    2.使用load_state_dict()把参数加载到模型中\n",
    "    3.修改输出层\n",
    "    4.固定 feature extractor 的参数。这部分通常有 2 种做法：\n",
    "        4.1固定卷积层的预训练参数。可以设置requires_grad=False或者lr=0\n",
    "        4.2可以通过params_group给 feature extractor 设置一个较小的学习率"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 下面微调 ResNet18，用于蜜蜂和蚂蚁图片的二分类。训练集每类数据各 120 张，验证集每类数据各 70 张图片。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import os\n",
    "import random\n",
    "import torch\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset\n",
    "import numpy as np\n",
    "\n",
    "random.seed(1)\n",
    "rmb_label = {\"1\": 0, \"100\": 1}\n",
    "ants_label={'ants':0, 'bees':1}\n",
    "\n",
    "class RMBDataset(Dataset):\n",
    "    def __init__(self, data_dir, transform=None):\n",
    "        \"\"\"\n",
    "        rmb面额分类任务的Dataset\n",
    "        :param data_dir: str, 数据集所在路径\n",
    "        :param transform: torch.transform，数据预处理\n",
    "        \"\"\"\n",
    "        # data_info存储所有图片路径和标签，在DataLoader中通过index读取样本\n",
    "        self.data_info = self.get_img_info(data_dir)\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # 通过 index 读取样本\n",
    "        path_img, label = self.data_info[index]\n",
    "        # 注意这里需要 convert('RGB')\n",
    "        img = Image.open(path_img).convert('RGB')     # 0~255\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)   # 在这里做transform，转为tensor等等\n",
    "        # 返回是样本和标签\n",
    "        return img, label\n",
    "\n",
    "    # 返回所有样本的数量\n",
    "    def __len__(self):\n",
    "        return len(self.data_info)\n",
    "\n",
    "    @staticmethod\n",
    "    def get_img_info(data_dir):\n",
    "        data_info = list()\n",
    "        # data_dir 是训练集、验证集或者测试集的路径\n",
    "        for root, dirs, _ in os.walk(data_dir):\n",
    "            # 遍历类别\n",
    "            # dirs ['1', '100']\n",
    "            for sub_dir in dirs:\n",
    "                # 文件列表\n",
    "                img_names = os.listdir(os.path.join(root, sub_dir))\n",
    "                # 取出 jpg 结尾的文件\n",
    "                img_names = list(filter(lambda x: x.endswith('.jpg'), img_names))\n",
    "                # 遍历图片\n",
    "                for i in range(len(img_names)):\n",
    "                    img_name = img_names[i]\n",
    "                    # 图片的绝对路径\n",
    "                    path_img = os.path.join(root, sub_dir, img_name)\n",
    "                    # 标签，这里需要映射为 0、1 两个类别\n",
    "                    label = rmb_label[sub_dir]\n",
    "                    # 保存在 data_info 变量中\n",
    "                    data_info.append((path_img, int(label)))\n",
    "        return data_info\n",
    "\n",
    "class AntsDataset(Dataset):\n",
    "    def __init__(self,data_dir, transform=None):\n",
    "        self.label_name={'ants':0, 'bees':1}\n",
    "        self.data_info=self.get_item_info(data_dir)\n",
    "        self.transform=transform\n",
    "\n",
    "    def  __getitem__(self, index):\n",
    "        path,label=self.data_info[index]\n",
    "        img = Image.open(path).convert('RGB')\n",
    "        if self.transform is not None:\n",
    "            img=self.transform(img)\n",
    "        return img, label\n",
    "\n",
    "    @staticmethod\n",
    "    def get_item_info(data_dir):\n",
    "        data_info=list()\n",
    "        for root,dirs,_ in os.walk(data_dir):\n",
    "            for sub_dir in dirs:\n",
    "                img_names=os.listdir(os.path.join(root,sub_dir))\n",
    "                img_names=list(filter(lambda x:x.endswith('.jpg'), img_names))\n",
    "\n",
    "                for i in range(len(img_names)):\n",
    "                    path_img=os.path.join(root,sub_dir,img_names[i])\n",
    "                    label=ants_label[sub_dir]\n",
    "                    data_info.append((path_img, int(label)))\n",
    "\n",
    "        if len(data_info)==0:\n",
    "            raise Exception('\\ndata_dir:{} is a empty dir! please check your image paths!'.format(data_dir))\n",
    "\n",
    "        return data_info\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_info)\n",
    "    \n",
    "def set_seed(seed=1):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "模型finetune方法\n",
    "\"\"\"\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "import torch.optim as optim\n",
    "from matplotlib import pyplot as plt\n",
    "from lesson2.rmb_classification.tools.my_dataset import AntsDataset\n",
    "from common_tools import set_seed\n",
    "import torchvision.models as models\n",
    "import enviroments\n",
    "BASEDIR = os.path.dirname(os.path.abspath(__file__))\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"use device :{}\".format(device))\n",
    "\n",
    "set_seed(1)  # 设置随机种子\n",
    "label_name = {\"ants\": 0, \"bees\": 1}\n",
    "\n",
    "# 参数设置\n",
    "MAX_EPOCH = 25\n",
    "BATCH_SIZE = 16\n",
    "LR = 0.001\n",
    "log_interval = 10\n",
    "val_interval = 1\n",
    "classes = 2\n",
    "start_epoch = -1\n",
    "lr_decay_step = 7\n",
    "\n",
    "\n",
    "# ============================ step 1/5 数据 ============================\n",
    "data_dir = enviroments.hymenoptera_data_dir\n",
    "train_dir = os.path.join(data_dir, \"train\")\n",
    "valid_dir = os.path.join(data_dir, \"val\")\n",
    "\n",
    "norm_mean = [0.485, 0.456, 0.406]\n",
    "norm_std = [0.229, 0.224, 0.225]\n",
    "\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(224),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(norm_mean, norm_std),\n",
    "])\n",
    "\n",
    "valid_transform = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(norm_mean, norm_std),\n",
    "])\n",
    "\n",
    "# 构建MyDataset实例\n",
    "train_data = AntsDataset(data_dir=train_dir, transform=train_transform)\n",
    "valid_data = AntsDataset(data_dir=valid_dir, transform=valid_transform)\n",
    "\n",
    "# 构建DataLoder\n",
    "train_loader = DataLoader(dataset=train_data, batch_size=BATCH_SIZE, shuffle=True)\n",
    "valid_loader = DataLoader(dataset=valid_data, batch_size=BATCH_SIZE)\n",
    "\n",
    "# ============================ step 2/5 模型 ============================\n",
    "\n",
    "# 1/3 构建模型\n",
    "resnet18_ft = models.resnet18()\n",
    "\n",
    "# 2/3 加载参数\n",
    "# flag = 0\n",
    "flag = 1\n",
    "if flag:\n",
    "    path_pretrained_model = enviroments.resnet18_path\n",
    "    state_dict_load = torch.load(path_pretrained_model)\n",
    "    resnet18_ft.load_state_dict(state_dict_load)\n",
    "\n",
    "# 法1 : 冻结卷积层\n",
    "flag_m1 = 0\n",
    "# flag_m1 = 1\n",
    "if flag_m1:\n",
    "    for param in resnet18_ft.parameters():\n",
    "        param.requires_grad = False\n",
    "    # print(\"conv1.weights[0, 0, ...]:\\n {}\".format(resnet18_ft.conv1.weight[0, 0, ...]))\n",
    "\n",
    "\n",
    "# 3/3 替换fc层\n",
    "# 首先拿到 fc 层的输入个数\n",
    "num_ftrs = resnet18_ft.fc.in_features\n",
    "# 然后构造新的 fc 层替换原来的 fc 层\n",
    "resnet18_ft.fc = nn.Linear(num_ftrs, classes)\n",
    "\n",
    "\n",
    "resnet18_ft.to(device)\n",
    "# ============================ step 3/5 损失函数 ============================\n",
    "criterion = nn.CrossEntropyLoss()                                                   # 选择损失函数\n",
    "\n",
    "# ============================ step 4/5 优化器 ============================\n",
    "# 法2 : conv 小学习率\n",
    "flag = 0\n",
    "# flag = 1\n",
    "if flag:\n",
    "    # 首先获取全连接层参数的地址\n",
    "    fc_params_id = list(map(id, resnet18_ft.fc.parameters()))  # 返回的是parameters的 内存地址\n",
    "    # 然后使用 filter 过滤不属于全连接层的参数，也就是保留卷积层的参数\n",
    "    base_params = filter(lambda p: id(p) not in fc_params_id, resnet18_ft.parameters())\n",
    "    # 设置优化器的分组学习率，传入一个 list，包含 2 个元素，每个元素是字典。对应 2 个参数组\n",
    "    optimizer = optim.SGD([{'params': base_params, 'lr': LR * 0.1}, {'params': resnet18_ft.fc.parameters(), 'lr': LR}],\n",
    "                          momentum=0.9)\n",
    "\n",
    "else:\n",
    "    optimizer = optim.SGD(resnet18_ft.parameters(), lr=LR, momentum=0.9)               # 选择优化器\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=lr_decay_step, gamma=0.1)     # 设置学习率下降策略\n",
    "\n",
    "\n",
    "# ============================ step 5/5 训练 ============================\n",
    "train_curve = list()\n",
    "valid_curve = list()\n",
    "\n",
    "for epoch in range(start_epoch + 1, MAX_EPOCH):\n",
    "\n",
    "    loss_mean = 0.\n",
    "    correct = 0.\n",
    "    total = 0.\n",
    "\n",
    "    resnet18_ft.train()\n",
    "    for i, data in enumerate(train_loader):\n",
    "\n",
    "        # forward\n",
    "        inputs, labels = data\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        outputs = resnet18_ft(inputs)\n",
    "\n",
    "        # backward\n",
    "        optimizer.zero_grad()\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "\n",
    "        # update weights\n",
    "        optimizer.step()\n",
    "\n",
    "        # 统计分类情况\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).squeeze().cpu().sum().numpy()\n",
    "\n",
    "        # 打印训练信息\n",
    "        loss_mean += loss.item()\n",
    "        train_curve.append(loss.item())\n",
    "        if (i+1) % log_interval == 0:\n",
    "            loss_mean = loss_mean / log_interval\n",
    "            print(\"Training:Epoch[{:0>3}/{:0>3}] Iteration[{:0>3}/{:0>3}] Loss: {:.4f} Acc:{:.2%}\".format(\n",
    "                epoch, MAX_EPOCH, i+1, len(train_loader), loss_mean, correct / total))\n",
    "            loss_mean = 0.\n",
    "\n",
    "            # if flag_m1:\n",
    "            # print(\"epoch:{} conv1.weights[0, 0, ...] :\\n {}\".format(epoch, resnet18_ft.conv1.weight[0, 0, ...]))\n",
    "\n",
    "    scheduler.step()  # 更新学习率\n",
    "\n",
    "    # validate the model\n",
    "    if (epoch+1) % val_interval == 0:\n",
    "\n",
    "        correct_val = 0.\n",
    "        total_val = 0.\n",
    "        loss_val = 0.\n",
    "        resnet18_ft.eval()\n",
    "        with torch.no_grad():\n",
    "            for j, data in enumerate(valid_loader):\n",
    "                inputs, labels = data\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "                outputs = resnet18_ft(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total_val += labels.size(0)\n",
    "                correct_val += (predicted == labels).squeeze().cpu().sum().numpy()\n",
    "\n",
    "                loss_val += loss.item()\n",
    "\n",
    "            loss_val_mean = loss_val/len(valid_loader)\n",
    "            valid_curve.append(loss_val_mean)\n",
    "            print(\"Valid:\\t Epoch[{:0>3}/{:0>3}] Iteration[{:0>3}/{:0>3}] Loss: {:.4f} Acc:{:.2%}\".format(\n",
    "                epoch, MAX_EPOCH, j+1, len(valid_loader), loss_val_mean, correct_val / total_val))\n",
    "        resnet18_ft.train()\n",
    "\n",
    "train_x = range(len(train_curve))\n",
    "train_y = train_curve\n",
    "\n",
    "train_iters = len(train_loader)\n",
    "valid_x = np.arange(1, len(valid_curve)+1) * train_iters*val_interval # 由于valid中记录的是epochloss，需要对记录点进行转换到iterations\n",
    "valid_y = valid_curve\n",
    "\n",
    "plt.plot(train_x, train_y, label='Train')\n",
    "plt.plot(valid_x, valid_y, label='Valid')\n",
    "\n",
    "plt.legend(loc='upper right')\n",
    "plt.ylabel('loss value')\n",
    "plt.xlabel('Iteration')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 不使用 Finetune\n",
    "```\n",
    "第一次我们首先不使用 Finetune，而是从零开始训练模型，这时只需要修改全连接层即可：\n",
    "```\n",
    "```python\n",
    "# 首先拿到 fc 层的输入个数\n",
    "num_ftrs = resnet18_ft.fc.in_features\n",
    "# 然后构造新的 fc 层替换原来的 fc 层\n",
    "resnet18_ft.fc = nn.Linear(num_ftrs, classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 使用 Finetune\n",
    "```\n",
    "然后我们把下载的模型参数加载到模型中：\n",
    "```\n",
    "```python\n",
    "path_pretrained_model = enviroments.resnet18_path\n",
    "state_dict_load = torch.load(path_pretrained_model)\n",
    "resnet18_ft.load_state_dict(state_dict_load)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 冻结卷积层\n",
    "```\n",
    "设置requires_grad=False\n",
    "\n",
    "这里先冻结所有参数，然后再替换全连接层，相当于冻结了卷积层的参数：\n",
    "```\n",
    "```python\n",
    "for param in resnet18_ft.parameters():\n",
    "    param.requires_grad = False\n",
    "# 首先拿到 fc 层的输入个数\n",
    "num_ftrs = resnet18_ft.fc.in_features\n",
    "# 然后构造新的 fc 层替换原来的 fc 层\n",
    "resnet18_ft.fc = nn.Linear(num_ftrs, classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 设置学习率为 0\n",
    "```\n",
    "这里把卷积层的学习率设置为 0，需要在优化器里设置不同的学习率。首先获取全连接层参数的地址，然后使用 filter 过滤不属于全连接层的参数，也就是保留卷积层的参数；接着设置优化器的分组学习率，传入一个 list，包含 2 个元素，每个元素是字典，对应 2 个参数组。其中卷积层的学习率设置为 全连接层的 0.1 倍。\n",
    "```\n",
    "```python\n",
    "# 首先获取全连接层参数的地址\n",
    "fc_params_id = list(map(id, resnet18_ft.fc.parameters()))     # 返回的是parameters的 内存地址\n",
    "# 然后使用 filter 过滤不属于全连接层的参数，也就是保留卷积层的参数\n",
    "base_params = filter(lambda p: id(p) not in fc_params_id, resnet18_ft.parameters())\n",
    "# 设置优化器的分组学习率，传入一个 list，包含 2 个元素，每个元素是字典，对应 2 个参数组\n",
    "optimizer = optim.SGD([{'params': base_params, 'lr': 0}, {'params': resnet18_ft.fc.parameters(), 'lr': LR}], momentum=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 使用分组学习率\n",
    "```\n",
    "这里不冻结卷积层，而是对卷积层使用较小的学习率，对全连接层使用较大的学习率，需要在优化器里设置不同的学习率。首先获取全连接层参数的地址，然后使用 filter 过滤不属于全连接层的参数，也就是保留卷积层的参数；接着设置优化器的分组学习率，传入一个 list，包含 2 个元素，每个元素是字典，对应 2 个参数组。其中卷积层的学习率设置为 全连接层的 0.1 倍。\n",
    "```\n",
    "```python\n",
    "# 首先获取全连接层参数的地址\n",
    "fc_params_id = list(map(id, resnet18_ft.fc.parameters()))     # 返回的是parameters的 内存地址\n",
    "# 然后使用 filter 过滤不属于全连接层的参数，也就是保留卷积层的参数\n",
    "base_params = filter(lambda p: id(p) not in fc_params_id, resnet18_ft.parameters())\n",
    "# 设置优化器的分组学习率，传入一个 list，包含 2 个元素，每个元素是字典，对应 2 个参数组\n",
    "optimizer = optim.SGD([{'params': base_params, 'lr': LR*0}, {'params': resnet18_ft.fc.parameters(), 'lr': LR}], momentum=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 使用 GPU 的 tips"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "PyTorch 模型使用 GPU，可以分为 3 步：\n",
    "    1.首先获取 device：device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    2.把模型加载到 device：model.to(device)\n",
    "    3.在 data_loader 取数据的循环中，把每个 mini-batch 的数据和 label 加载到 device：inputs, labels = inputs.to(device), labels.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('pytorch_env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6ee8edfac98a68d22388d5c9d2c882fbbce04e225b817d7a8b0ca882dcedeab3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
