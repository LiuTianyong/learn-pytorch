{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 人民币 二分类"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "实现 1 元人民币和 100 元人民币的图片二分类。前面讲过 PyTorch 的五大模块：数据、模型、损失函数、优化器和迭代训练。\n",
    "数据模块又可以细分为 4 个部分：\n",
    "数据收集：样本和标签。\n",
    "数据划分：训练集、验证集和测试集\n",
    "数据读取：对应于PyTorch 的 DataLoader。其中 DataLoader 包括 Sampler 和 DataSet。Sampler 的功能是生成索引， DataSet 是根据生成的索引读取样本以及标签。\n",
    "数据预处理：对应于 PyTorch 的 transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataLoader 与 DataSet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "DataLoader()\n",
    "torch.utils.data.DataLoader(dataset, batch_size=1, shuffle=False, sampler=None, batch_sampler=None, num_workers=0, collate_fn=None, pin_memory=False, drop_last=False, timeout=0, worker_init_fn=None, multiprocessing_context=None)\n",
    "```\n",
    "```\n",
    "功能：构建可迭代的数据装载器\n",
    "dataset: Dataset 类，决定数据从哪里读取以及如何读取\n",
    "batchsize: 批大小\n",
    "num_works:num_works: 是否多进程读取数据\n",
    "sheuffle: 每个 epoch 是否乱序\n",
    "drop_last: 当样本数不能被 batchsize 整除时，是否舍弃最后一批数据\n",
    "\n",
    "Epoch, Iteration, Batchsize\n",
    "Epoch: 所有训练样本都已经输入到模型中，称为一个 Epoch\n",
    "Iteration: 一批样本输入到模型中，称为一个 Iteration\n",
    "Batchsize: 批大小，决定一个 iteration 有多少样本，也决定了一个 Epoch 有多少个 Iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "DataSet()\n",
    "```\n",
    "```\n",
    "功能：Dataset 是抽象类，所有自定义的 Dataset 都需要继承该类，并且重写__getitem()__方法和__len__()方法 。__getitem()__方法的作用是接收一个索引，返回索引对应的样本和标签，这是我们自己需要实现的逻辑。__len__()方法是返回所有样本的数量。\n",
    "数据读取包含 3 个方面\n",
    "读取哪些数据：每个 Iteration 读取一个 Batchsize 大小的数据，每个 Iteration 应该读取哪些数据。\n",
    "从哪里读取数据：如何找到硬盘中的数据，应该在哪里设置文件路径参数\n",
    "如何读取数据：不同的文件需要使用不同的读取方法和库。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tianyongliu/miniforge3/envs/pytorch_env/lib/python3.8/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: dlopen(/Users/tianyongliu/miniforge3/envs/pytorch_env/lib/python3.8/site-packages/torchvision/image.so, 0x0006): symbol not found in flat namespace '__ZN3c106detail19maybe_wrap_dim_slowExxb'\n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import shutil\n",
    "import torch\n",
    "from PIL import Image\n",
    "\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import numpy as np\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from tensorboardX import  SummaryWriter\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建文件夹\n",
    "def makedir(new_dir):\n",
    "    if not os.path.exists(new_dir):\n",
    "        os.makedirs(new_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(1)\n",
    "\n",
    "# 'data\\\\RMB_data'\n",
    "dataset_dir = os.path.join(\"../data\", \"RMB_data\")\n",
    "\n",
    "# 'data\\\\rmb_split'\n",
    "split_dir = os.path.join(\"data\", \"rmb_split\")\n",
    "\n",
    "# 'data\\\\rmb_split\\\\train'\n",
    "train_dir = os.path.join(split_dir, \"train\")\n",
    "# 'data\\\\rmb_split\\\\valid'\n",
    "valid_dir = os.path.join(split_dir, \"valid\")\n",
    "# 'data\\\\rmb_split\\\\test'\n",
    "test_dir = os.path.join(split_dir, \"test\")\n",
    "\n",
    "# 训练集\n",
    "train_pct = 0.8\n",
    "# 验证集\n",
    "valid_pct = 0.1\n",
    "# 测试集\n",
    "test_pct = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../data/RMB_data'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class:1, train:0, valid:0, test:0\n",
      "Class:100, train:0, valid:0, test:0\n"
     ]
    }
   ],
   "source": [
    "# 划分训练集验证集 测试集\n",
    "for root, dirs, files in os.walk(dataset_dir):\n",
    "    # 文件列表\n",
    "    # dirs: ['1', '100']\n",
    "    for sub_dir in dirs:\n",
    "        imgs = os.listdir(os.path.join(root, sub_dir))\n",
    "        # 取出 jpg 结尾的文件\n",
    "        imgs = list(filter(lambda x: x.endswith('.jpg'), imgs))\n",
    "        random.shuffle(imgs)\n",
    "        \n",
    "        # 计算图片数量\n",
    "        img_count = len(imgs)\n",
    "        # 计算训练集索引的结束位置\n",
    "        train_point = int(img_count * train_pct)\n",
    "        # 计算验证集索引的结束位置\n",
    "        valid_point = int(img_count * (train_pct + valid_pct))\n",
    "        \n",
    "        # 把数据划分到训练集、验证集、测试集的文件夹\n",
    "        for i in range(img_count):\n",
    "            if i < train_point:\n",
    "                out_dir = os.path.join(train_dir, sub_dir)\n",
    "            elif i < valid_point:\n",
    "                out_dir = os.path.join(valid_dir, sub_dir)\n",
    "            else:\n",
    "                out_dir = os.path.join(test_dir, sub_dir)\n",
    "                \n",
    "            # 创建文件夹\n",
    "            makedir(out_dir)\n",
    "            # 构造目标文件名\n",
    "            target_path = os.path.join(out_dir, imgs[i])\n",
    "            # 构造源文件名\n",
    "            src_path = os.path.join(dataset_dir, sub_dir, imgs[i])\n",
    "            # 复制\n",
    "            shutil.copy(src_path, target_path)\n",
    "        print('Class:{}, train:{}, valid:{}, test:{}'.format(sub_dir, train_point, valid_point-train_point,\n",
    "                                                                 img_count-valid_point))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(1)\n",
    "rmb_label = {\"1\": 0, \"100\": 1}\n",
    "\n",
    "class RMBDataset(Dataset):\n",
    "    def __init__(self, data_dir, transform=None):\n",
    "        \"\"\"\n",
    "        rmb面额分类任务的Dataset\n",
    "        :param data_dir: str, 数据集所在路径\n",
    "        :param transform: torch.transform，数据预处理\n",
    "        \"\"\"\n",
    "        # data_info存储所有图片路径和标签，在DataLoader中通过index读取样本\n",
    "        self.data_info = self.get_img_info(data_dir)\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # 通过 index 读取样本\n",
    "        path_img, label = self.data_info[index]\n",
    "        # 注意这里需要 convert('RGB')\n",
    "        img = Image.open(path_img).convert('RGB')     # 0~255\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)   # 在这里做transform，转为tensor等等\n",
    "        # 返回是样本和标签\n",
    "        return img, label\n",
    "\n",
    "    # 返回所有样本的数量\n",
    "    def __len__(self):\n",
    "        return len(self.data_info)\n",
    "\n",
    "    @staticmethod\n",
    "    def get_img_info(data_dir):\n",
    "        data_info = list()\n",
    "        # data_dir 是训练集、验证集或者测试集的路径\n",
    "        for root, dirs, _ in os.walk(data_dir):\n",
    "            # 遍历类别\n",
    "            # dirs ['1', '100']\n",
    "            for sub_dir in dirs:\n",
    "                # 文件列表\n",
    "                img_names = os.listdir(os.path.join(root, sub_dir))\n",
    "                # 取出 jpg 结尾的文件\n",
    "                img_names = list(filter(lambda x: x.endswith('.jpg'), img_names))\n",
    "                # 遍历图片\n",
    "                for i in range(len(img_names)):\n",
    "                    img_name = img_names[i]\n",
    "                    # 图片的绝对路径\n",
    "                    path_img = os.path.join(root, sub_dir, img_name)\n",
    "                    # 标签，这里需要映射为 0、1 两个类别\n",
    "                    label = rmb_label[sub_dir]\n",
    "                    # 保存在 data_info 变量中\n",
    "                    data_info.append((path_img, int(label)))\n",
    "        return data_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LeNet(nn.Module):\n",
    "    # 子模块创建\n",
    "    def __init__(self, classes):\n",
    "        super(LeNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, classes)\n",
    "\n",
    "    # 子模块拼接\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.conv1(x))\n",
    "        out = F.max_pool2d(out, 2)\n",
    "        out = F.relu(self.conv2(out))\n",
    "        out = F.max_pool2d(out, 2)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = F.relu(self.fc1(out))\n",
    "        out = F.relu(self.fc2(out))\n",
    "        out = self.fc3(out)\n",
    "        return out\n",
    "\n",
    "    def initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.xavier_normal_(m.weight.data)\n",
    "                if m.bias is not None:\n",
    "                    m.bias.data.zero_()\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                m.weight.data.fill_(1)\n",
    "                m.bias.data.zero_()\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                nn.init.normal_(m.weight.data, 0, 0.1)\n",
    "                m.bias.data.zero_()\n",
    "\n",
    "\n",
    "class LeNetSequetial(nn.Module):\n",
    "    def __init__(self, classes):\n",
    "        super(LeNetSequetial, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 6, 5),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.Conv2d(6, 16, 5),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2)\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(16*5*5, 120),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(120, 84),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(84, classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = x.view(x.size()[0], -1)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "    def initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.xavier_normal_(m.weight.data)\n",
    "                if m.bias is not None:\n",
    "                    m.bias.data.zero_()\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                m.weight.data.fill_(1)\n",
    "                m.bias.data.zero_()\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                nn.init.normal_(m.weight.data, 0, 0.1)\n",
    "                m.bias.data.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 设置随机种子\n",
    "def set_seed(seed=1):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(921)  # 设置随机种子\n",
    "rmb_label = {\"1\": 0, \"100\": 1}\n",
    "\n",
    "# 参数设置\n",
    "MAX_EPOCH = 10\n",
    "BATCH_SIZE = 16\n",
    "LR = 0.01\n",
    "log_interval = 10\n",
    "val_interval = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================ step 1/5 数据 ============================\n",
    "\n",
    "# 设置路径参数\n",
    "rmb_split_dir = 'data/rmb_split'\n",
    "\n",
    "train_dir = os.path.join(rmb_split_dir, \"train\")\n",
    "valid_dir = os.path.join(rmb_split_dir, \"valid\")\n",
    "\n",
    "norm_mean = [0.485, 0.456, 0.406]\n",
    "norm_std = [0.229, 0.224, 0.225]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 设置训练集的数据增强和转化\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((32, 32)),\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(norm_mean, norm_std),\n",
    "])\n",
    "\n",
    "# 设置验证集的数据增强和转化，不需要 RandomCrop\n",
    "valid_transform = transforms.Compose([\n",
    "    transforms.Resize((32, 32)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(norm_mean, norm_std),\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 构建MyDataset实例\n",
    "train_data = RMBDataset(data_dir=train_dir, transform=train_transform)\n",
    "valid_data = RMBDataset(data_dir=valid_dir, transform=valid_transform)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 构建DataLoder\n",
    "# 其中训练集设置 shuffle=True，表示每个 Epoch 都打乱样本\n",
    "train_loader = DataLoader(dataset=train_data, batch_size=BATCH_SIZE, shuffle=True)\n",
    "valid_loader = DataLoader(dataset=valid_data, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================ step 2/5 模型 ============================\n",
    "\n",
    "net = LeNetSequetial(classes=2)\n",
    "net.initialize_weights()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================ step 3/5 损失函数 ============================\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()    # 选择损失函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================ step 4/5 优化器 ============================\n",
    "\n",
    "optimizer = optim.SGD(net.parameters(), lr=LR, momentum=0.9)                        # 选择优化器\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)     # 设置学习率下降策略"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================ step 5/5 训练 ============================\n",
    "\n",
    "train_curve = list()\n",
    "valid_curve = list()\n",
    "\n",
    "iter_count = 0\n",
    "\n",
    "# 构建 SummaryWriter\n",
    "writer = SummaryWriter(comment='test_your_comment', filename_suffix=\"_test_your_filename_suffix\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [18], line 10\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[39m# 遍历 train_loader 取数据\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[39mfor\u001b[39;00m i, data \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(train_loader):\n\u001b[0;32m---> 10\u001b[0m     \u001b[39mprint\u001b[39m(data\u001b[39m.\u001b[39;49mshape)\n\u001b[1;32m     11\u001b[0m     iter_count \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m     12\u001b[0m     \u001b[39m# forward\u001b[39;00m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "for epoch in range(MAX_EPOCH):\n",
    "\n",
    "    loss_mean = 0.\n",
    "    correct = 0.\n",
    "    total = 0.\n",
    "\n",
    "    net.train()\n",
    "    # 遍历 train_loader 取数据\n",
    "    for i, data in enumerate(train_loader):\n",
    "        print(data)\n",
    "        iter_count += 1\n",
    "        # forward\n",
    "        inputs, labels = data\n",
    "        outputs = net(inputs)\n",
    "\n",
    "        # backward\n",
    "        optimizer.zero_grad()\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "\n",
    "        # update weights\n",
    "        optimizer.step()\n",
    "\n",
    "        # 统计分类情况\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).squeeze().sum().numpy()\n",
    "\n",
    "        # 打印训练信息\n",
    "        loss_mean += loss.item()\n",
    "        train_curve.append(loss.item())\n",
    "        if (i+1) % log_interval == 0:\n",
    "            loss_mean = loss_mean / log_interval\n",
    "            print(\"Training:Epoch[{:0>3}/{:0>3}] Iteration[{:0>3}/{:0>3}] Loss: {:.4f} Acc:{:.2%}\".format(\n",
    "                epoch, MAX_EPOCH, i+1, len(train_loader), loss_mean, correct / total))\n",
    "            loss_mean = 0.\n",
    "\n",
    "        # 记录数据，保存于event file\n",
    "        writer.add_scalars(\"Loss\", {\"Train\": loss.item()}, iter_count)\n",
    "        writer.add_scalars(\"Accuracy\", {\"Train\": correct / total}, iter_count)\n",
    "\n",
    "    # 每个epoch，记录梯度，权值\n",
    "    for name, param in net.named_parameters():\n",
    "        writer.add_histogram(name + '_grad', param.grad, epoch)\n",
    "        writer.add_histogram(name + '_data', param, epoch)\n",
    "\n",
    "    scheduler.step()  # 每个 epoch 更新学习率\n",
    "    # 每个 epoch 计算验证集得准确率和loss\n",
    "    # validate the model\n",
    "    if (epoch+1) % val_interval == 0:\n",
    "\n",
    "        correct_val = 0.\n",
    "        total_val = 0.\n",
    "        loss_val = 0.\n",
    "        net.eval()\n",
    "        with torch.no_grad():\n",
    "            for j, data in enumerate(valid_loader):\n",
    "                inputs, labels = data\n",
    "                outputs = net(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total_val += labels.size(0)\n",
    "                correct_val += (predicted == labels).squeeze().sum().numpy()\n",
    "\n",
    "                loss_val += loss.item()\n",
    "\n",
    "            valid_curve.append(loss_val/valid_loader.__len__())\n",
    "            print(\"Valid:\\t Epoch[{:0>3}/{:0>3}] Iteration[{:0>3}/{:0>3}] Loss: {:.4f} Acc:{:.2%}\".format(\n",
    "                epoch, MAX_EPOCH, j+1, len(valid_loader), loss_val, correct_val / total_val))\n",
    "\n",
    "            # 记录数据，保存于event file\n",
    "            writer.add_scalars(\"Loss\", {\"Valid\": np.mean(valid_curve)}, iter_count)\n",
    "            writer.add_scalars(\"Accuracy\", {\"Valid\": correct / total}, iter_count)\n",
    "\n",
    "train_x = range(len(train_curve))\n",
    "train_y = train_curve\n",
    "\n",
    "train_iters = len(train_loader)\n",
    "valid_x = np.arange(1, len(valid_curve)+1) * train_iters*val_interval # 由于valid中记录的是epochloss，需要对记录点进行转换到iterations\n",
    "valid_y = valid_curve\n",
    "\n",
    "plt.plot(train_x, train_y, label='Train')\n",
    "plt.plot(valid_x, valid_y, label='Valid')\n",
    "\n",
    "plt.legend(loc='upper right')\n",
    "plt.ylabel('loss value')\n",
    "plt.xlabel('Iteration')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('pytorch_env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13 | packaged by conda-forge | (default, Mar 25 2022, 06:05:16) \n[Clang 12.0.1 ]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6ee8edfac98a68d22388d5c9d2c882fbbce04e225b817d7a8b0ca882dcedeab3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
