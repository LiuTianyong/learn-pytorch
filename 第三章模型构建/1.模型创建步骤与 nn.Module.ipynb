{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 网络模型的创建步骤\n",
    "创建模型有 2 个要素：构建子模块和拼接子模块"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 总结\n",
    "```\n",
    "一个 module 里可包含多个子 module。比如 LeNet 是一个 Module，里面包括多个卷积层、池化层、全连接层等子 module\n",
    "一个 module 相当于一个运算，必须实现 forward() 函数\n",
    "每个 module 都有 8 个字典管理自己的属性"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LeNet(nn.Module):\n",
    "    # 子模块创建\n",
    "    def __init__(self, classes):\n",
    "        super(LeNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(16*5*5, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, classes)\n",
    "    # 子模块拼接\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.conv1(x))\n",
    "        out = F.max_pool2d(out, 2)\n",
    "        out = F.relu(self.conv2(out))\n",
    "        out = F.max_pool2d(out, 2)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = F.relu(self.fc1(out))\n",
    "        out = F.relu(self.fc2(out))\n",
    "        out = self.fc3(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 模型容器\n",
    "```\n",
    "除了上述的模块之外，还有一个重要的概念是模型容器 (Containers)，常用的容器有 3 个，这些容器都是继承自nn.Module。\n",
    "    nn.Sequetial：按照顺序包装多个网络层\n",
    "    nn.ModuleList：像 python 的 list 一样包装多个网络层，可以迭代\n",
    "    nn.ModuleDict：像 python 的 dict一样包装多个网络层，通过 (key, value) 的方式为每个网络层指定名称。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### nn.Sequetial\n",
    "```\n",
    "在传统的机器学习中，有一个步骤是特征工程，我们需要从数据中认为地提取特征，然后把特征输入到分类器中预测。在深度学习的时代，特征工程的概念被弱化了，特征提取和分类器这两步被融合到了一个神经网络中。在卷积神经网络中，前面的卷积层以及池化层可以认为是特征提取部分，而后面的全连接层可以认为是分类器部分。比如 LeNet 就可以分为特征提取和分类器两部分，这 2 部分都可以分别使用 nn.Seuqtial 来包装。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LeNetSequetial(nn.Module):\n",
    "    def __init__(self, classes):\n",
    "        super(LeNet, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 6, 5),\n",
    "            nn.ReLU(),\n",
    "            nn.AvgPool2d(2, 2),\n",
    "            nn.Conv2d(6, 16, 5),\n",
    "            nn.ReLU(),\n",
    "            nn.AvgPool2d(2, 2)\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(16*5*5, 120),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(120, 84),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(84, classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = x.view(x.size()[0], -1)\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LeNetSequentialOrderDict(nn.Module):\n",
    "    def __init__(self, classes):\n",
    "        super(LeNetSequentialOrderDict, self).__init__()\n",
    "\n",
    "        self.features = nn.Sequential(OrderedDict({\n",
    "            'conv1': nn.Conv2d(3, 6, 5),\n",
    "            'relu1': nn.ReLU(inplace=True),\n",
    "            'pool1': nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "\n",
    "            'conv2': nn.Conv2d(6, 16, 5),\n",
    "            'relu2': nn.ReLU(inplace=True),\n",
    "            'pool2': nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "        }))\n",
    "\n",
    "        self.classifier = nn.Sequential(OrderedDict({\n",
    "            'fc1': nn.Linear(16*5*5, 120),\n",
    "            'relu3': nn.ReLU(),\n",
    "\n",
    "            'fc2': nn.Linear(120, 84),\n",
    "            'relu4': nn.ReLU(inplace=True),\n",
    "\n",
    "            'fc3': nn.Linear(84, classes),\n",
    "        }))\n",
    "        ...\n",
    "        ...\n",
    "        ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 总结\n",
    "```\n",
    "nn.Sequetial是nn.Module的容器，用于按顺序包装一组网络层，有以下两个特性。\n",
    "顺序性：各网络层之间严格按照顺序构建，我们在构建网络时，一定要注意前后网络层之间输入和输出数据之间的形状是否匹配\n",
    "自带forward()函数：在nn.Sequetial的forward()函数里通过 for 循环依次读取每个网络层，执行前向传播运算。这使得我们我们构建的模型更加简洁"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### nn.ModuleList\n",
    "```\n",
    "nn.ModuleList是nn.Module的容器，用于包装一组网络层，以迭代的方式调用网络层，主要有以下 3 个方法：\n",
    "append()：在 ModuleList 后面添加网络层\n",
    "extend()：拼接两个 ModuleList\n",
    "insert()：在 ModuleList 的指定位置中插入网络层\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ModuleList(\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=10, out_features=10, bias=True)\n",
      "    (1): Linear(in_features=10, out_features=10, bias=True)\n",
      "    (2): Linear(in_features=10, out_features=10, bias=True)\n",
      "    (3): Linear(in_features=10, out_features=10, bias=True)\n",
      "    (4): Linear(in_features=10, out_features=10, bias=True)\n",
      "    (5): Linear(in_features=10, out_features=10, bias=True)\n",
      "    (6): Linear(in_features=10, out_features=10, bias=True)\n",
      "    (7): Linear(in_features=10, out_features=10, bias=True)\n",
      "    (8): Linear(in_features=10, out_features=10, bias=True)\n",
      "    (9): Linear(in_features=10, out_features=10, bias=True)\n",
      "    (10): Linear(in_features=10, out_features=10, bias=True)\n",
      "    (11): Linear(in_features=10, out_features=10, bias=True)\n",
      "    (12): Linear(in_features=10, out_features=10, bias=True)\n",
      "    (13): Linear(in_features=10, out_features=10, bias=True)\n",
      "    (14): Linear(in_features=10, out_features=10, bias=True)\n",
      "    (15): Linear(in_features=10, out_features=10, bias=True)\n",
      "    (16): Linear(in_features=10, out_features=10, bias=True)\n",
      "    (17): Linear(in_features=10, out_features=10, bias=True)\n",
      "    (18): Linear(in_features=10, out_features=10, bias=True)\n",
      "    (19): Linear(in_features=10, out_features=10, bias=True)\n",
      "  )\n",
      ")\n",
      "tensor([[-0.1503,  0.2270,  0.0423, -0.2037,  0.0061,  0.1066, -0.5398,  0.5678,\n",
      "         -0.1778, -0.1493],\n",
      "        [-0.1503,  0.2270,  0.0423, -0.2037,  0.0061,  0.1066, -0.5398,  0.5678,\n",
      "         -0.1778, -0.1493],\n",
      "        [-0.1503,  0.2270,  0.0423, -0.2037,  0.0061,  0.1066, -0.5398,  0.5678,\n",
      "         -0.1778, -0.1493],\n",
      "        [-0.1503,  0.2270,  0.0423, -0.2037,  0.0061,  0.1066, -0.5398,  0.5678,\n",
      "         -0.1778, -0.1493],\n",
      "        [-0.1503,  0.2270,  0.0423, -0.2037,  0.0061,  0.1066, -0.5398,  0.5678,\n",
      "         -0.1778, -0.1493],\n",
      "        [-0.1503,  0.2270,  0.0423, -0.2037,  0.0061,  0.1066, -0.5398,  0.5678,\n",
      "         -0.1778, -0.1493],\n",
      "        [-0.1503,  0.2270,  0.0423, -0.2037,  0.0061,  0.1066, -0.5398,  0.5678,\n",
      "         -0.1778, -0.1493],\n",
      "        [-0.1503,  0.2270,  0.0423, -0.2037,  0.0061,  0.1066, -0.5398,  0.5678,\n",
      "         -0.1778, -0.1493],\n",
      "        [-0.1503,  0.2270,  0.0423, -0.2037,  0.0061,  0.1066, -0.5398,  0.5678,\n",
      "         -0.1778, -0.1493],\n",
      "        [-0.1503,  0.2270,  0.0423, -0.2037,  0.0061,  0.1066, -0.5398,  0.5678,\n",
      "         -0.1778, -0.1493]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "class ModuleList(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ModuleList, self).__init__()\n",
    "        self.linears = nn.ModuleList([nn.Linear(10, 10) for i in range(20)])\n",
    "\n",
    "    def forward(self, x):\n",
    "        for i, linear in enumerate(self.linears):\n",
    "            x = linear(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "net = ModuleList()\n",
    "\n",
    "print(net)\n",
    "\n",
    "fake_data = torch.ones((10, 10))\n",
    "\n",
    "output = net(fake_data)\n",
    "\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### nn.ModuleDict\n",
    "```\n",
    "nn.ModuleDict是nn.Module的容器，用于包装一组网络层，以索引的方式调用网络层，主要有以下 5 个方法：\n",
    "clear()：清空  ModuleDict\n",
    "items()：返回可迭代的键值对 (key, value)\n",
    "keys()：返回字典的所有 key\n",
    "values()：返回字典的所有 value\n",
    "pop()：返回一对键值，并从字典中删除"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[0.0000, 0.4500, 0.0000,  ..., 0.0000, 0.0000, 0.1617],\n",
      "          [0.1225, 0.0000, 1.0683,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          [0.9547, 0.1126, 0.0000,  ..., 0.0000, 0.2309, 0.5861],\n",
      "          ...,\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.3741, 0.0000, 0.0000],\n",
      "          [0.0000, 0.2046, 0.3119,  ..., 0.0121, 1.1352, 0.0000],\n",
      "          [0.0000, 0.3488, 0.1335,  ..., 0.0000, 0.2153, 0.0000]],\n",
      "\n",
      "         [[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.6400, 0.0000],\n",
      "          [0.4010, 0.6984, 0.1598,  ..., 0.0000, 0.0000, 0.3244],\n",
      "          [0.0000, 0.0000, 0.3402,  ..., 0.9861, 0.2562, 0.0000],\n",
      "          ...,\n",
      "          [0.0000, 0.6298, 0.0616,  ..., 0.2296, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.7999],\n",
      "          [0.0000, 0.4847, 0.1579,  ..., 0.1189, 0.0000, 0.0000]],\n",
      "\n",
      "         [[0.0000, 0.9329, 0.0000,  ..., 0.0825, 0.3141, 0.1213],\n",
      "          [0.4767, 0.0000, 0.8529,  ..., 0.0000, 0.0000, 0.4342],\n",
      "          [0.0000, 0.0138, 0.0000,  ..., 0.0000, 0.7200, 0.3844],\n",
      "          ...,\n",
      "          [0.1925, 0.6009, 0.3408,  ..., 0.8058, 0.0000, 0.0000],\n",
      "          [0.6315, 0.0000, 0.3135,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.1658,  ..., 0.3529, 0.0130, 0.0000]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[0.7227, 0.8213, 0.8870,  ..., 0.3520, 0.0000, 0.4576],\n",
      "          [0.0000, 0.0000, 0.2988,  ..., 0.0000, 0.0000, 0.3718],\n",
      "          [1.0747, 0.0000, 0.4573,  ..., 0.0000, 0.5180, 1.8417],\n",
      "          ...,\n",
      "          [0.3117, 1.2545, 0.1761,  ..., 0.6974, 0.4825, 0.0000],\n",
      "          [0.4952, 1.0963, 0.8644,  ..., 0.0000, 0.4078, 0.0000],\n",
      "          [0.0000, 0.0000, 0.5196,  ..., 0.0000, 0.4541, 0.4898]],\n",
      "\n",
      "         [[0.0000, 0.0947, 0.1520,  ..., 0.0000, 0.6179, 0.2164],\n",
      "          [0.0949, 0.3100, 0.6108,  ..., 0.0000, 0.3106, 0.4000],\n",
      "          [0.0000, 0.3932, 0.0000,  ..., 0.0000, 0.3230, 0.0000],\n",
      "          ...,\n",
      "          [0.6806, 0.2238, 0.0000,  ..., 0.8841, 0.0000, 0.0000],\n",
      "          [0.5114, 0.2904, 0.2105,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.2403, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.5304, 0.3102, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          [0.7898, 0.1148, 0.4430,  ..., 0.1848, 0.0000, 0.2369],\n",
      "          [0.0000, 0.0000, 0.6772,  ..., 0.1313, 0.0000, 0.4412],\n",
      "          ...,\n",
      "          [0.0000, 0.0000, 0.7744,  ..., 0.6082, 0.0000, 0.0000],\n",
      "          [0.4056, 1.0390, 0.4543,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.2552, 0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[0.4348, 0.0000, 0.3371,  ..., 0.0510, 0.5290, 0.0000],\n",
      "          [0.8116, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.1917, 0.4853,  ..., 0.0392, 0.0120, 0.0000],\n",
      "          ...,\n",
      "          [0.5034, 1.0774, 0.0000,  ..., 0.5602, 0.4278, 0.4400],\n",
      "          [0.2950, 0.0861, 0.0000,  ..., 1.5243, 0.3307, 0.0000],\n",
      "          [0.0000, 0.3626, 0.0813,  ..., 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[0.0000, 0.0659, 0.6056,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.7538, 0.2756, 0.0000],\n",
      "          [0.2455, 0.2186, 0.0000,  ..., 0.5902, 0.0000, 0.0000],\n",
      "          ...,\n",
      "          [0.0000, 0.9150, 0.0218,  ..., 0.2439, 0.0000, 0.0000],\n",
      "          [0.0000, 0.3312, 0.0000,  ..., 0.0000, 0.2974, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.6463, 0.4050, 0.0000]],\n",
      "\n",
      "         [[0.4236, 0.0000, 0.3220,  ..., 0.4657, 0.0000, 0.4965],\n",
      "          [0.3576, 0.1890, 0.1970,  ..., 0.0000, 0.0000, 0.3684],\n",
      "          [0.4319, 1.0347, 0.0000,  ..., 0.0000, 0.0000, 0.2537],\n",
      "          ...,\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.5502, 0.6794,  ..., 0.0000, 0.0197, 0.2410],\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0857, 0.0000]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[0.2026, 0.3652, 0.0000,  ..., 0.8417, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.6041, 0.0311],\n",
      "          [0.0000, 0.2263, 0.2647,  ..., 0.5083, 0.0000, 0.7506],\n",
      "          ...,\n",
      "          [0.0967, 0.0000, 0.0000,  ..., 0.3131, 0.0000, 0.0000],\n",
      "          [0.0000, 0.1188, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          [0.6564, 0.3136, 0.4918,  ..., 0.4171, 0.6101, 0.0000]],\n",
      "\n",
      "         [[0.0000, 0.9917, 0.6076,  ..., 0.5138, 0.0000, 0.0000],\n",
      "          [0.7285, 0.8156, 0.6605,  ..., 0.0000, 0.0000, 0.2589],\n",
      "          [0.5025, 0.1081, 0.0000,  ..., 0.0000, 0.0000, 0.4115],\n",
      "          ...,\n",
      "          [0.2639, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          [0.2263, 0.2925, 0.0000,  ..., 0.0000, 0.2786, 0.0000],\n",
      "          [0.0000, 0.0000, 0.1133,  ..., 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[0.0000, 0.2687, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.2534, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.1149, 0.0000],\n",
      "          ...,\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.3903, 0.0000, 0.0000],\n",
      "          [0.8904, 1.3104, 0.0000,  ..., 0.0819, 0.6546, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.2636]]],\n",
      "\n",
      "\n",
      "        [[[0.4249, 0.0000, 0.0685,  ..., 0.2700, 0.0000, 0.0826],\n",
      "          [0.3090, 0.4296, 0.0000,  ..., 0.0000, 0.0913, 0.5402],\n",
      "          [0.0000, 0.2097, 0.1295,  ..., 0.0000, 0.0000, 0.5082],\n",
      "          ...,\n",
      "          [0.3269, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          [0.7319, 0.0000, 1.0806,  ..., 0.3603, 0.0000, 0.1213],\n",
      "          [0.0733, 0.7753, 0.8141,  ..., 0.1703, 0.0000, 0.8097]],\n",
      "\n",
      "         [[0.5034, 0.0000, 0.0000,  ..., 0.0000, 0.9080, 0.4380],\n",
      "          [0.8871, 0.7572, 0.8282,  ..., 0.0000, 0.4933, 0.0000],\n",
      "          [0.0000, 0.0000, 0.3244,  ..., 0.0168, 0.2681, 0.0000],\n",
      "          ...,\n",
      "          [0.2570, 0.0000, 0.3341,  ..., 0.4907, 0.5705, 0.5823],\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.3027, 0.0000],\n",
      "          [0.2591, 0.0000, 0.0000,  ..., 0.0000, 0.2157, 0.8430]],\n",
      "\n",
      "         [[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.1918, 0.0000,  ..., 0.0000, 0.0000, 1.2631],\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0239, 0.0000],\n",
      "          ...,\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0695],\n",
      "          [0.4695, 0.0000, 0.0036,  ..., 0.0000, 0.0000, 0.5414],\n",
      "          [0.2449, 0.0000, 0.0000,  ..., 0.0909, 0.8828, 0.0229]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[0.9207, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.8769],\n",
      "          [0.0000, 0.8266, 0.0000,  ..., 0.0000, 0.0000, 0.3470],\n",
      "          [0.0000, 0.0000, 0.2195,  ..., 0.0000, 0.3660, 0.0000],\n",
      "          ...,\n",
      "          [1.0390, 0.8709, 0.0000,  ..., 0.9036, 0.0000, 0.1683],\n",
      "          [0.0000, 0.0860, 0.0622,  ..., 0.0000, 0.7329, 1.1353],\n",
      "          [0.0000, 1.0571, 0.0000,  ..., 0.0000, 0.0000, 0.5619]],\n",
      "\n",
      "         [[0.1191, 0.0000, 0.0000,  ..., 0.0000, 0.4605, 0.2823],\n",
      "          [0.0000, 0.2725, 0.0000,  ..., 0.2213, 0.4898, 0.3699],\n",
      "          [0.1902, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          ...,\n",
      "          [0.0000, 0.3545, 0.5991,  ..., 0.0040, 0.0000, 0.1590],\n",
      "          [0.0000, 0.1309, 0.8263,  ..., 0.3698, 0.0000, 0.5146],\n",
      "          [0.0000, 0.0000, 0.1258,  ..., 0.0000, 0.4255, 0.2373]],\n",
      "\n",
      "         [[0.3239, 0.0000, 0.0000,  ..., 0.0524, 0.5743, 0.0000],\n",
      "          [0.0000, 0.7130, 0.5438,  ..., 0.0000, 1.2115, 0.2993],\n",
      "          [0.0538, 0.0000, 0.4033,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          ...,\n",
      "          [0.0000, 0.0000, 0.1641,  ..., 0.1158, 0.0000, 0.6488],\n",
      "          [0.0000, 0.0000, 0.0781,  ..., 0.4282, 0.4303, 0.1269],\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.2635, 0.6367, 0.0288]]],\n",
      "\n",
      "\n",
      "        [[[0.0000, 0.0000, 1.1255,  ..., 0.4163, 2.0434, 0.0000],\n",
      "          [0.7081, 0.2833, 0.3505,  ..., 0.0023, 0.7278, 0.7712],\n",
      "          [0.4301, 0.3770, 0.0000,  ..., 0.0000, 0.3731, 1.2590],\n",
      "          ...,\n",
      "          [0.3903, 0.0000, 0.2296,  ..., 0.0000, 0.0000, 0.9672],\n",
      "          [0.6586, 0.3689, 0.0000,  ..., 0.0000, 0.0000, 0.3125],\n",
      "          [0.8768, 0.0000, 0.5574,  ..., 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[0.0000, 0.0000, 0.7266,  ..., 0.1304, 0.0000, 0.7456],\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.1907, 0.0000,  ..., 1.0495, 0.0000, 1.1733],\n",
      "          ...,\n",
      "          [0.3514, 0.0000, 0.0000,  ..., 0.8381, 0.4827, 0.0000],\n",
      "          [0.5222, 0.6563, 0.0106,  ..., 0.0000, 0.7249, 0.1517],\n",
      "          [0.0736, 0.0000, 0.5471,  ..., 0.2491, 0.5423, 0.0000]],\n",
      "\n",
      "         [[0.0000, 0.2797, 0.0000,  ..., 0.3910, 0.0000, 0.7232],\n",
      "          [0.8727, 0.0000, 0.0000,  ..., 0.0000, 0.8003, 0.6151],\n",
      "          [0.0000, 0.0000, 0.0059,  ..., 0.0000, 0.5710, 0.0000],\n",
      "          ...,\n",
      "          [0.6404, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          [0.8667, 0.0000, 0.0000,  ..., 0.0000, 0.7823, 0.0000],\n",
      "          [0.5122, 0.0000, 0.0000,  ..., 0.3001, 0.0000, 0.0000]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[0.0000, 0.0000, 0.7286,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          [0.5080, 0.6096, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 1.2131, 0.0000,  ..., 0.1279, 0.0000, 0.0813],\n",
      "          ...,\n",
      "          [0.3754, 1.1668, 0.0000,  ..., 0.4119, 0.0000, 0.0000],\n",
      "          [0.5180, 0.0000, 0.0000,  ..., 0.0000, 0.3831, 0.0000],\n",
      "          [0.9650, 0.0000, 0.0000,  ..., 0.2809, 1.4434, 0.3353]],\n",
      "\n",
      "         [[0.0000, 0.2718, 0.0481,  ..., 0.8570, 0.3274, 0.0000],\n",
      "          [0.1855, 0.0000, 0.5983,  ..., 0.8093, 0.5161, 0.0000],\n",
      "          [0.3439, 0.9692, 0.6568,  ..., 0.0000, 0.3658, 0.0000],\n",
      "          ...,\n",
      "          [0.7139, 0.0000, 0.0000,  ..., 0.2421, 0.3322, 0.0922],\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.5324, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[0.0000, 0.5992, 0.0000,  ..., 0.8813, 0.0000, 0.5454],\n",
      "          [0.0000, 0.0959, 0.8075,  ..., 0.0000, 0.3132, 0.0000],\n",
      "          [0.0000, 0.2242, 0.0000,  ..., 0.8286, 0.0000, 0.0000],\n",
      "          ...,\n",
      "          [0.4561, 0.0000, 0.0000,  ..., 0.0000, 1.1842, 0.0000],\n",
      "          [0.1069, 1.0299, 0.0000,  ..., 0.0000, 0.5013, 0.2812],\n",
      "          [0.0000, 0.0000, 0.6654,  ..., 0.6055, 0.0000, 0.0000]]]],\n",
      "       grad_fn=<ReluBackward0>)\n"
     ]
    }
   ],
   "source": [
    "class ModuleDict(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ModuleDict, self).__init__()\n",
    "        self.choices = nn.ModuleDict({\n",
    "            'conv': nn.Conv2d(10, 10, 3),\n",
    "            'pool': nn.MaxPool2d(3)\n",
    "        })\n",
    "\n",
    "        self.activations = nn.ModuleDict({\n",
    "            'relu': nn.ReLU(),\n",
    "            'prelu': nn.PReLU()\n",
    "        })\n",
    "\n",
    "    def forward(self, x, choice, act):\n",
    "        x = self.choices[choice](x)\n",
    "        x = self.activations[act](x)\n",
    "        return x\n",
    "\n",
    "\n",
    "net = ModuleDict()\n",
    "\n",
    "fake_img = torch.randn((4, 10, 32, 32))\n",
    "\n",
    "output = net(fake_img, 'conv', 'relu')\n",
    "# output = net(fake_img, 'conv', 'prelu')\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 容器总结\n",
    "```\n",
    "nn.Sequetial：顺序性，各网络层之间严格按照顺序执行，常用于 block 构建，在前向传播时的代码调用变得简洁\n",
    "nn.ModuleList：迭代行，常用于大量重复网络构建，通过 for 循环实现重复构建\n",
    "nn.ModuleDict：索引性，常用于可选择的网络层"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PyTorch 中的 AlexNet\n",
    "```\n",
    "AlexNet 是 Hinton 和他的学生等人在 2012 年提出的卷积神经网络，以高出第二名 10 多个百分点的准确率获得 ImageNet 分类任务冠军，从此卷积神经网络开始在世界上流行，是划时代的贡献。\n",
    "AlexNet 特点如下：\n",
    "采用 ReLU 替换饱和激活 函数，减轻梯度消失\n",
    "采用 LRN (Local Response Normalization) 对数据进行局部归一化，减轻梯度消失\n",
    "采用 Dropout 提高网络的鲁棒性，增加泛化能力\n",
    "使用 Data Augmentation，包括 TenCrop 和一些色彩修改\n",
    "```\n",
    "![AlexNet](https://image.zhangxiann.com/20200614162004.png \"AlexNet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AlexNet(nn.Module):\n",
    "\n",
    "    def __init__(self, num_classes=1000):\n",
    "        super(AlexNet, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=11, stride=4, padding=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "            \n",
    "            nn.Conv2d(64, 192, kernel_size=5, padding=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "            \n",
    "            nn.Conv2d(192, 384, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            \n",
    "            nn.Conv2d(384, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            \n",
    "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "        )\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((6, 6))\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(256 * 6 * 6, 4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(4096, 4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(4096, num_classes),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('pytorch_env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6ee8edfac98a68d22388d5c9d2c882fbbce04e225b817d7a8b0ca882dcedeab3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
