{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hook 函数概念\n",
    "Hook 函数是在不改变主体的情况下，实现额外功能。由于 PyTorch 是基于动态图实现的，因此在一次迭代运算结束后，一些中间变量如非叶子节点的梯度和特征图，会被释放掉。在这种情况下想要提取和记录这些中间变量，就需要使用 Hook 函数。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### torch.Tensor.register_hook(hook)\n",
    "```\n",
    "功能：注册一个反向传播 hook 函数，仅输入一个参数，为张量的梯度。\n",
    "hook函数：\n",
    "```\n",
    "```python\n",
    "hook(grad)\n",
    "```\n",
    "参数：\n",
    "grad：张量的梯度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gradient: tensor([5.]) tensor([2.]) None None None\n",
      "a_grad[0]:  tensor([2.])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "w = torch.tensor([1.], requires_grad=True)\n",
    "x = torch.tensor([2.], requires_grad=True)\n",
    "a = torch.add(w, x)\n",
    "b = torch.add(w, 1)\n",
    "y = torch.mul(a, b)\n",
    "\n",
    "# 保存梯度的 list\n",
    "a_grad = list()\n",
    "\n",
    "# 定义 hook 函数，把梯度添加到 list 中\n",
    "def grad_hook(grad):\n",
    "    a_grad.append(grad)\n",
    "\n",
    "# 一个张量注册 hook 函数\n",
    "handle = a.register_hook(grad_hook)\n",
    "\n",
    "y.backward()\n",
    "\n",
    "# 查看梯度\n",
    "print(\"gradient:\", w.grad, x.grad, a.grad, b.grad, y.grad)\n",
    "# 查看在 hook 函数里 list 记录的梯度\n",
    "print(\"a_grad[0]: \", a_grad[0])\n",
    "\n",
    "handle.remove()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w.grad:  tensor([30.])\n"
     ]
    }
   ],
   "source": [
    "# hook函数里面可以修改梯度的值，无需返回也可以作为新的梯度赋值给原来的梯度。代码如下：\n",
    "w = torch.tensor([1.], requires_grad=True)\n",
    "x = torch.tensor([2.], requires_grad=True)\n",
    "a = torch.add(w, x)\n",
    "b = torch.add(w, 1)\n",
    "y = torch.mul(a, b)\n",
    "\n",
    "a_grad = list()\n",
    "\n",
    "def grad_hook(grad):\n",
    "    grad *= 2\n",
    "    return grad*3\n",
    "\n",
    "handle = w.register_hook(grad_hook)\n",
    "\n",
    "y.backward()\n",
    "\n",
    "# 查看梯度\n",
    "print(\"w.grad: \", w.grad)\n",
    "handle.remove()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### torch.nn.Module.register_forward_hook(hook)\n",
    "```\n",
    "功能：注册 module 的前向传播hook函数，可用于获取中间的 feature map。\n",
    "hook函数：\n",
    "```\n",
    "```python\n",
    "hook(module, input, output)\n",
    "```\n",
    "```\n",
    "参数：\n",
    "    module：当前网络层\n",
    "    input：当前网络层输入数据\n",
    "    output：当前网络层输出数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output shape: torch.Size([1, 2, 1, 1])\n",
      "output value: tensor([[[[ 9.]],\n",
      "\n",
      "         [[18.]]]], grad_fn=<MaxPool2DWithIndicesBackward0>)\n",
      "\n",
      "feature maps shape: torch.Size([1, 2, 2, 2])\n",
      "output value: tensor([[[[ 9.,  9.],\n",
      "          [ 9.,  9.]],\n",
      "\n",
      "         [[18., 18.],\n",
      "          [18., 18.]]]], grad_fn=<ConvolutionBackward0>)\n",
      "\n",
      "input shape: torch.Size([1, 1, 4, 4])\n",
      "input value: (tensor([[[[1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1.]]]]),)\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 2, 3)\n",
    "        self.pool1 = nn.MaxPool2d(2, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.pool1(x)\n",
    "        return x\n",
    "\n",
    "def forward_hook(module, data_input, data_output):\n",
    "    fmap_block.append(data_output)\n",
    "    input_block.append(data_input)\n",
    "\n",
    "# 初始化网络\n",
    "net = Net()\n",
    "net.conv1.weight[0].detach().fill_(1)\n",
    "net.conv1.weight[1].detach().fill_(2)\n",
    "net.conv1.bias.data.detach().zero_()\n",
    "\n",
    "# 注册hook\n",
    "fmap_block = list()\n",
    "input_block = list()\n",
    "net.conv1.register_forward_hook(forward_hook)\n",
    "\n",
    "# inference\n",
    "fake_img = torch.ones((1, 1, 4, 4))   # batch size * channel * H * W\n",
    "output = net(fake_img)\n",
    "\n",
    "\n",
    "# 观察\n",
    "print(\"output shape: {}\\noutput value: {}\\n\".format(output.shape, output))\n",
    "\n",
    "print(\"feature maps shape: {}\\noutput value: {}\\n\".format(fmap_block[0].shape, fmap_block[0]))\n",
    "\n",
    "print(\"input shape: {}\\ninput value: {}\".format(input_block[0][0].shape, input_block[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### torch.Tensor.register_forward_pre_hook()\n",
    "```\n",
    "功能：注册 module 的前向传播前的hook函数，可用于获取输入数据。\n",
    "```\n",
    "```python\n",
    "hook(module, input)\n",
    "```\n",
    "```\n",
    "参数：\n",
    "    module：当前网络层\n",
    "    input：当前网络层输入数据\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### torch.Tensor.register_backward_hook()\n",
    "```\n",
    "功能：注册 module 的反向传播的hook函数，可用于获取梯度。\n",
    "```\n",
    "```python\n",
    "hook(module, grad_input, grad_output)\n",
    "```\n",
    "```\n",
    "参数：\n",
    "    module：当前网络层\n",
    "    input：当前网络层输入的梯度数据\n",
    "    output：当前网络层输出的梯度数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "forward_pre_hook input:(tensor([[[[1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1.]]]]),)\n",
      "backward hook input:(None, tensor([[[[0.5000, 0.5000, 0.5000],\n",
      "          [0.5000, 0.5000, 0.5000],\n",
      "          [0.5000, 0.5000, 0.5000]]],\n",
      "\n",
      "\n",
      "        [[[0.5000, 0.5000, 0.5000],\n",
      "          [0.5000, 0.5000, 0.5000],\n",
      "          [0.5000, 0.5000, 0.5000]]]]), tensor([0.5000, 0.5000]))\n",
      "backward hook output:(tensor([[[[0.5000, 0.0000],\n",
      "          [0.0000, 0.0000]],\n",
      "\n",
      "         [[0.5000, 0.0000],\n",
      "          [0.0000, 0.0000]]]]),)\n"
     ]
    }
   ],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 2, 3)\n",
    "        self.pool1 = nn.MaxPool2d(2, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.pool1(x)\n",
    "        return x\n",
    "\n",
    "def forward_hook(module, data_input, data_output):\n",
    "    fmap_block.append(data_output)\n",
    "    input_block.append(data_input)\n",
    "\n",
    "def forward_pre_hook(module, data_input):\n",
    "    print(\"forward_pre_hook input:{}\".format(data_input))\n",
    "\n",
    "def backward_hook(module, grad_input, grad_output):\n",
    "    print(\"backward hook input:{}\".format(grad_input))\n",
    "    print(\"backward hook output:{}\".format(grad_output))\n",
    "\n",
    "# 初始化网络\n",
    "net = Net()\n",
    "net.conv1.weight[0].detach().fill_(1)\n",
    "net.conv1.weight[1].detach().fill_(2)\n",
    "net.conv1.bias.data.detach().zero_()\n",
    "\n",
    "# 注册hook\n",
    "fmap_block = list()\n",
    "input_block = list()\n",
    "net.conv1.register_forward_hook(forward_hook)\n",
    "net.conv1.register_forward_pre_hook(forward_pre_hook)\n",
    "net.conv1.register_backward_hook(backward_hook)\n",
    "\n",
    "# inference\n",
    "fake_img = torch.ones((1, 1, 4, 4))   # batch size * channel * H * W\n",
    "output = net(fake_img)\n",
    "\n",
    "loss_fnc = nn.L1Loss()\n",
    "target = torch.randn_like(output)\n",
    "loss = loss_fnc(target, output)\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### hook函数实现机制\n",
    "```\n",
    "hook函数实现的原理是在module的__call()__函数进行拦截，__call()__函数可以分为 4 个部分：\n",
    "第 1 部分是实现 _forward_pre_hooks\n",
    "第 2 部分是实现 forward 前向传播 \n",
    "第 3 部分是实现 _forward_hooks  \n",
    "第 4 部分是实现 _backward_hooks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "def __call__(self, *input, **kwargs):\n",
    "    # 第 1 部分是实现 _forward_pre_hooks\n",
    "    for hook in self._forward_pre_hooks.values():\n",
    "        result = hook(self, input)\n",
    "        if result is not None:\n",
    "            if not isinstance(result, tuple):\n",
    "                result = (result,)\n",
    "            input = result\n",
    "\n",
    "    # 第 2 部分是实现 forward 前向传播       \n",
    "    if torch._C._get_tracing_state():\n",
    "        result = self._slow_forward(*input, **kwargs)\n",
    "    else:\n",
    "        result = self.forward(*input, **kwargs)\n",
    "\n",
    "    # 第 3 部分是实现 _forward_hooks   \n",
    "    for hook in self._forward_hooks.values():\n",
    "        hook_result = hook(self, input, result)\n",
    "        if hook_result is not None:\n",
    "            result = hook_result\n",
    "\n",
    "    # 第 4 部分是实现 _backward_hooks\n",
    "    if len(self._backward_hooks) > 0:\n",
    "        var = result\n",
    "        while not isinstance(var, torch.Tensor):\n",
    "            if isinstance(var, dict):\n",
    "                var = next((v for v in var.values() if isinstance(v, torch.Tensor)))\n",
    "            else:\n",
    "                var = var[0]\n",
    "        grad_fn = var.grad_fn\n",
    "        if grad_fn is not None:\n",
    "            for hook in self._backward_hooks.values():\n",
    "                wrapper = functools.partial(hook, self)\n",
    "                functools.update_wrapper(wrapper, hook)\n",
    "                grad_fn.register_hook(wrapper)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hook 函数提取网络的特征图\n",
    "```\n",
    "下面通过hook函数获取 AlexNet 每个卷积层的所有卷积核参数，以形状作为 key，value 对应该层多个卷积核的 list。然后取出每层的第一个卷积核，形状是 [1, in_channle, h, w]，转换为 [in_channle, 1, h, w]，使用 TensorBoard 进行可视化，代码如下："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "writer = SummaryWriter(comment='test_your_comment', filename_suffix=\"_test_your_filename_suffix\")\n",
    "\n",
    "# 数据\n",
    "path_img = \"imgs/lena.png\"     # your path to image\n",
    "normMean = [0.49139968, 0.48215827, 0.44653124]\n",
    "normStd = [0.24703233, 0.24348505, 0.26158768]\n",
    "\n",
    "norm_transform = transforms.Normalize(normMean, normStd)\n",
    "img_transforms = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    norm_transform\n",
    "])\n",
    "\n",
    "img_pil = Image.open(path_img).convert('RGB')\n",
    "if img_transforms is not None:\n",
    "    img_tensor = img_transforms(img_pil)\n",
    "img_tensor.unsqueeze_(0)    # chw --> bchw\n",
    "\n",
    "# 模型\n",
    "alexnet = models.alexnet(pretrained=True)\n",
    "\n",
    "# 注册hook\n",
    "fmap_dict = dict()\n",
    "for name, sub_module in alexnet.named_modules():\n",
    "\n",
    "    if isinstance(sub_module, nn.Conv2d):\n",
    "        key_name = str(sub_module.weight.shape)\n",
    "        fmap_dict.setdefault(key_name, list())\n",
    "        # 由于AlexNet 使用 nn.Sequantial 包装，所以 name 的形式是：features.0  features.1\n",
    "        n1, n2 = name.split(\".\")\n",
    "\n",
    "        def hook_func(m, i, o):\n",
    "            key_name = str(m.weight.shape)\n",
    "            fmap_dict[key_name].append(o)\n",
    "\n",
    "        alexnet._modules[n1]._modules[n2].register_forward_hook(hook_func)\n",
    "\n",
    "# forward\n",
    "output = alexnet(img_tensor)\n",
    "\n",
    "# add image\n",
    "for layer_name, fmap_list in fmap_dict.items():\n",
    "    fmap = fmap_list[0]# 取出第一个卷积核的参数\n",
    "    fmap.transpose_(0, 1) # 把 BCHW 转换为 CBHW\n",
    "\n",
    "    nrow = int(np.sqrt(fmap.shape[0]))\n",
    "    fmap_grid = vutils.make_grid(fmap, normalize=True, scale_each=True, nrow=nrow)\n",
    "    writer.add_image('feature map in {}'.format(layer_name), fmap_grid, global_step=322)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('pytorch_env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6ee8edfac98a68d22388d5c9d2c882fbbce04e225b817d7a8b0ca882dcedeab3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
